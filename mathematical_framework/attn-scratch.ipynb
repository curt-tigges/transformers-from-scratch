{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output matrix shape: (3, 12)\n",
      "[[5.30984289e+01 5.72017866e+01 5.69877472e+01 4.46998351e+01\n",
      "  6.52204369e+01 4.65477197e+01 4.96062475e+01 4.84270962e+01\n",
      "  5.46022068e+01 7.65112718e+01 3.47745563e+01 5.97145625e+01]\n",
      " [1.88563769e-10 2.01904032e-10 2.00673811e-10 1.57291810e-10\n",
      "  2.34654430e-10 1.69160248e-10 1.80991391e-10 1.71185050e-10\n",
      "  1.91803692e-10 2.77942069e-10 1.22604462e-10 2.10412911e-10]\n",
      " [6.31252502e-17 6.76329228e-17 6.72293265e-17 5.26723705e-17\n",
      "  7.86233398e-17 5.67073674e-17 6.06817541e-17 5.73296135e-17\n",
      "  6.42436082e-17 9.31473601e-17 4.10568552e-17 7.04968392e-17]]\n",
      "Refactored output matrix shape: (3, 12)\n",
      "[[5.30984289e+01 5.72017866e+01 5.69877472e+01 4.46998351e+01\n",
      "  6.52204369e+01 4.65477197e+01 4.96062475e+01 4.84270962e+01\n",
      "  5.46022068e+01 7.65112718e+01 3.47745563e+01 5.97145625e+01]\n",
      " [1.88563769e-10 2.01904032e-10 2.00673811e-10 1.57291810e-10\n",
      "  2.34654430e-10 1.69160248e-10 1.80991391e-10 1.71185050e-10\n",
      "  1.91803692e-10 2.77942069e-10 1.22604462e-10 2.10412911e-10]\n",
      " [6.31252502e-17 6.76329228e-17 6.72293265e-17 5.26723705e-17\n",
      "  7.86233398e-17 5.67073674e-17 6.06817541e-17 5.73296135e-17\n",
      "  6.42436082e-17 9.31473601e-17 4.10568552e-17 7.04968392e-17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def single_head_attention_with_weights(x, W_O, W_V, W_K, W_Q):\n",
    "    \"\"\"\n",
    "    Implements single-head attention mechanism with weight matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (n, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (n, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Query, Key, Value matrices from input and corresponding weight matrices (n x d_model)\n",
    "    Q = np.dot(x, W_Q)\n",
    "    K = np.dot(x, W_K)\n",
    "    V = np.dot(x, W_V)\n",
    "    \n",
    "    # Compute attention scores (n x n)\n",
    "    attention_scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Apply softmax to get attention distribution\n",
    "    attention_weights = softmax(attention_scores)\n",
    "    \n",
    "    # Compute weighted sum of value vectors (n x d_model)\n",
    "    weighted_sum = np.dot(attention_weights, V)\n",
    "    \n",
    "    # Apply the output weight matrix W_O (n x d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def single_head_attention_refactored(x, W_O, W_V, W_K, W_Q):\n",
    "    \"\"\"\n",
    "    Implements single-head attention mechanism with weight matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (n, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (n, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute attention scores\n",
    "    W_QK = np.dot(W_Q, W_K.T)\n",
    "    attention_scores = np.dot(np.dot(x, W_QK), x.T)\n",
    "    \n",
    "    # Apply softmax to get attention distribution\n",
    "    A = softmax(attention_scores)\n",
    "    \n",
    "\n",
    "    # Get W_V W_O transformation matrix\n",
    "    W_V_O = np.dot(W_V, W_O)\n",
    "    \n",
    "    # Compute output matrix\n",
    "    result = np.dot(A, np.dot(x, W_V_O))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "n = 3  # Number of tokens\n",
    "d_model = 12  # Dimensionality of each token\n",
    "\n",
    "# Randomly initialize input matrix and weight matrices\n",
    "x = np.random.rand(n, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Compute the output after single-head attention and linear transformations\n",
    "output = single_head_attention_with_weights(x, W_O, W_V, W_K, W_Q)\n",
    "print(\"Output matrix shape:\", output.shape)\n",
    "print(output)\n",
    "\n",
    "refactored_output = single_head_attention_refactored(x, W_O, W_V, W_K, W_Q)\n",
    "print(\"Refactored output matrix shape:\", refactored_output.shape)\n",
    "print(refactored_output)\n",
    "\n",
    "assert np.allclose(output, refactored_output), \"The refactored function produces different results!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original multi-head output matrix shape: (3, 12)\n",
      "[[13.11041448 14.45759892 17.24024685 19.25915873 17.07997889 14.14956077\n",
      "  11.3144583  19.49129843 15.59462499 17.40295627 17.59148781 20.68279173]\n",
      " [13.10060618 14.38749463 17.21519085 19.24719397 17.0751548  14.15010593\n",
      "  11.30695468 19.46354041 15.57782665 17.3633173  17.6274578  20.64612068]\n",
      " [13.14999024 14.48320425 17.25558089 19.30573325 17.11166652 14.18967852\n",
      "  11.36247037 19.52591501 15.64968297 17.49962161 17.6540556  20.76210828]]\n",
      "Refactored multi-head output matrix shape: (3, 12)\n",
      "[[13.11041448 14.45759892 17.24024685 19.25915873 17.07997889 14.14956077\n",
      "  11.3144583  19.49129843 15.59462499 17.40295627 17.59148781 20.68279173]\n",
      " [13.10060618 14.38749463 17.21519085 19.24719397 17.0751548  14.15010593\n",
      "  11.30695468 19.46354041 15.57782665 17.3633173  17.6274578  20.64612068]\n",
      " [13.14999024 14.48320425 17.25558089 19.30573325 17.11166652 14.18967852\n",
      "  11.36247037 19.52591501 15.64968297 17.49962161 17.6540556  20.76210828]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Splits the last dimension of x into (num_heads, d_head).\n",
    "    \"\"\"\n",
    "    seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    x = x.reshape(seq_length, num_heads, d_head)\n",
    "    return x.transpose(1, 0, 2)  # (num_heads, seq_length, d_head)\n",
    "\n",
    "def multi_head_attention_with_weights(x, W_O, W_V, W_K, W_Q, num_heads):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention mechanism with weight matrices using np.einsum.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (seq_length, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (seq_length, d_model)\n",
    "    \"\"\"\n",
    "    #print(x.shape, W_O.shape, W_V.shape, W_K.shape, W_Q.shape)\n",
    "    seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Compute Query, Key, Value matrices for each head\n",
    "    Q = split_heads(np.dot(x, W_Q), num_heads)  # (num_heads, seq_length, d_head)\n",
    "    K = split_heads(np.dot(x, W_K), num_heads)  # (num_heads, seq_length, d_head)\n",
    "    V = split_heads(np.dot(x, W_V), num_heads)  # (num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Compute attention scores and weights for each head\n",
    "    attention_scores = np.einsum('nqd,nkd->nqk', Q, K)  # (num_heads, seq_length, seq_length)\n",
    "    attention_weights = softmax(attention_scores / sqrt_d_head)\n",
    "    \n",
    "    # Compute weighted sum of value vectors for each head\n",
    "    weighted_sum = np.einsum('nqk,nkd->nqd', attention_weights, V)  # (num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Concatenate heads and apply the output weight matrix W_O\n",
    "    weighted_sum = weighted_sum.transpose(1, 0, 2).reshape(seq_length, d_model)  # (seq_length, d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def refactored_multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_head):\n",
    "    \"\"\"\n",
    "    Implements refactored multi-head attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input matrix of shape (seq_len, d_model)\n",
    "    - W_Q, W_K, W_V: Weight matrices for Query, Key, and Value, each of shape (d_model, d_model)\n",
    "    - W_O: Output weight matrix of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    - d_head: Dimension of each head\n",
    "\n",
    "    Returns:\n",
    "    - Output matrix after attention, of shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Initialize the result matrix\n",
    "    result = np.zeros((seq_len, d_model))\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        # Extract the weight matrices for the current head\n",
    "        W_Q_i = W_Q[:, i * d_head:(i + 1) * d_head]\n",
    "        W_K_i = W_K[:, i * d_head:(i + 1) * d_head]\n",
    "        W_V_i = W_V[:, i * d_head:(i + 1) * d_head]\n",
    "        W_O_i = W_O[i * d_head:(i + 1) * d_head, :]\n",
    "\n",
    "        # Compute the combined Query-Key weight matrix for the current head\n",
    "        W_QK_i = np.dot(W_Q_i, W_K_i.T)\n",
    "\n",
    "        # Compute the attention pattern\n",
    "        A = softmax(np.dot(np.dot(X, W_QK_i), X.T) / sqrt_d_head)\n",
    "        #print(f\"{A=}\")\n",
    "\n",
    "        # Compute the combined Value-Output weight matrix for the current head\n",
    "        W_VO_i = np.dot(W_V_i, W_O_i)\n",
    "        #print(f\"{W_VO_i=}\")\n",
    "\n",
    "        # Compute the result for the current head and add it to the overall result\n",
    "        XWV = np.dot(X, W_VO_i)\n",
    "        r_i = np.dot(A, XWV)\n",
    "        # print(f\"{r_i=}\")\n",
    "        # break\n",
    "        result += r_i\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "seq_len = 3\n",
    "d_model = 12\n",
    "num_heads = 3\n",
    "d_head = 4\n",
    "\n",
    "# Randomly initialize input matrix and weight matrices\n",
    "X = np.random.rand(seq_len, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Compute the output after normal multi-head attention\n",
    "output = multi_head_attention_with_weights(X, W_O, W_V, W_K, W_Q, num_heads)\n",
    "print(\"Original multi-head output matrix shape:\", output.shape)\n",
    "print(output)\n",
    "\n",
    "# Compute the output after refactored multi-head attention\n",
    "refactored_output = refactored_multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_head)\n",
    "print(\"Refactored multi-head output matrix shape:\", refactored_output.shape)\n",
    "print(refactored_output)\n",
    "\n",
    "assert np.allclose(output, refactored_output), \"The refactored function produces different results!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n",
      "[[[146.81187383 163.95858761 134.85684915 154.77074487 147.7483432\n",
      "   183.25228493 209.37117439 163.32475811 144.59898727 125.35787835\n",
      "   117.17723472 143.57535574 176.25787664 167.85282957 151.87931502\n",
      "   124.01945561 155.22062407 161.30247235 171.30248467 177.80024107]\n",
      "  [146.88605344 164.04509407 134.92249762 154.85118363 147.81875626\n",
      "   183.35077024 209.48031051 163.40620493 144.67221005 125.42087017\n",
      "   117.23783112 143.65309427 176.3454822  167.94105749 151.95675649\n",
      "   124.08419015 155.30505857 161.38253771 171.39291331 177.88518633]\n",
      "  [146.46580151 163.55742799 134.54858107 154.39717396 147.41749169\n",
      "   182.79696926 208.86486401 162.94305965 144.25748274 125.06482097\n",
      "   116.8960692  143.21650813 175.84878217 167.44471323 151.51912288\n",
      "   123.71903293 154.83066532 160.92819316 170.8837564  177.40123355]]\n",
      "\n",
      " [[111.28055273 123.94937867 101.8669829  117.02385175 111.7720689\n",
      "   138.80348445 158.43650219 123.16605511 109.57197323  94.90262272\n",
      "    88.90469643 108.66751154 133.41845157 127.42661297 115.04418536\n",
      "    93.84827726 117.91799495 122.08839324 129.91570162 134.49834497]\n",
      "  [111.99575005 124.77101241 102.50608953 117.78506658 112.47835448\n",
      "   139.72413154 159.4658366  123.94241882 110.27679803  95.50477171\n",
      "    89.49204925 109.395706   134.2716676  128.27027997 115.78989408\n",
      "    94.4444198  118.72273531 122.8534536  130.78311234 135.34583574]\n",
      "  [112.11451666 124.91042144 102.61700431 117.90995174 112.61277393\n",
      "   139.87651252 159.6410933  124.07871881 110.39285791  95.61157064\n",
      "    89.58720889 109.51310047 134.42591486 128.40620275 115.91162175\n",
      "    94.54504698 118.84998393 122.98929501 130.93019776 135.50019305]]]\n",
      "Refactored multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n",
      "[[[146.81187383 163.95858761 134.85684915 154.77074487 147.7483432\n",
      "   183.25228493 209.37117439 163.32475811 144.59898727 125.35787835\n",
      "   117.17723472 143.57535574 176.25787664 167.85282957 151.87931502\n",
      "   124.01945561 155.22062407 161.30247235 171.30248467 177.80024107]\n",
      "  [146.88605344 164.04509407 134.92249762 154.85118363 147.81875626\n",
      "   183.35077024 209.48031051 163.40620493 144.67221005 125.42087017\n",
      "   117.23783112 143.65309427 176.3454822  167.94105749 151.95675649\n",
      "   124.08419015 155.30505857 161.38253771 171.39291331 177.88518633]\n",
      "  [146.46580151 163.55742799 134.54858107 154.39717396 147.41749169\n",
      "   182.79696926 208.86486401 162.94305965 144.25748274 125.06482097\n",
      "   116.8960692  143.21650813 175.84878217 167.44471323 151.51912288\n",
      "   123.71903293 154.83066532 160.92819316 170.8837564  177.40123355]]\n",
      "\n",
      " [[111.28055273 123.94937867 101.8669829  117.02385175 111.7720689\n",
      "   138.80348445 158.43650219 123.16605511 109.57197323  94.90262272\n",
      "    88.90469643 108.66751154 133.41845157 127.42661297 115.04418536\n",
      "    93.84827726 117.91799495 122.08839324 129.91570162 134.49834497]\n",
      "  [111.99575005 124.77101241 102.50608953 117.78506658 112.47835448\n",
      "   139.72413154 159.4658366  123.94241882 110.27679803  95.50477171\n",
      "    89.49204925 109.395706   134.2716676  128.27027997 115.78989408\n",
      "    94.4444198  118.72273531 122.8534536  130.78311234 135.34583574]\n",
      "  [112.11451666 124.91042144 102.61700431 117.90995174 112.61277393\n",
      "   139.87651252 159.6410933  124.07871881 110.39285791  95.61157064\n",
      "    89.58720889 109.51310047 134.42591486 128.40620275 115.91162175\n",
      "    94.54504698 118.84998393 122.98929501 130.93019776 135.50019305]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Splits the last dimension of x into (num_heads, d_head) for a batch of sequences.\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    x = x.reshape(batch_size, seq_length, num_heads, d_head)\n",
    "    return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, d_head)\n",
    "\n",
    "def batched_classic_multi_head_attention(x, W_Q, W_K, W_V, W_O, num_heads):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention mechanism with weight matrices using np.einsum for a batch of sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Compute Query, Key, Value matrices for each head\n",
    "    Q = split_heads(np.dot(x, W_Q), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    K = split_heads(np.dot(x, W_K), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    V = split_heads(np.dot(x, W_V), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Compute attention scores and weights for each head\n",
    "    attention_scores = np.einsum('bnqd,bnkd->bnqk', Q, K)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "    attention_weights = softmax(attention_scores / sqrt_d_head)\n",
    "    #print(f\"{attention_weights=}\")\n",
    "    \n",
    "    # Compute weighted sum of value vectors for each head\n",
    "    weighted_sum = np.einsum('bnqk,bnkd->bnqd', attention_weights, V)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Concatenate heads and apply the output weight matrix W_O\n",
    "    weighted_sum = weighted_sum.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def batched_refactored_multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_head):\n",
    "    \"\"\"\n",
    "    Implements refactored multi-head attention mechanism for a batch of sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input matrix of shape (batch_size, seq_len, d_model)\n",
    "    - W_Q, W_K, W_V: Weight matrices for Query, Key, and Value, each of shape (d_model, d_model)\n",
    "    - W_O: Output weight matrix of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    - d_head: Dimension of each head\n",
    "\n",
    "    Returns:\n",
    "    - Output matrix after attention, of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = X.shape\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Initialize the result matrix\n",
    "    result = np.zeros((batch_size, seq_len, d_model))\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        # Extract the weight matrices for the current head\n",
    "        W_Q_i = W_Q[:, i * d_head:(i + 1) * d_head]\n",
    "        W_K_i = W_K[:, i * d_head:(i + 1) * d_head]\n",
    "        W_V_i = W_V[:, i * d_head:(i + 1) * d_head]\n",
    "        W_O_i = W_O[i * d_head:(i + 1) * d_head, :]\n",
    "\n",
    "        # Compute the combined Query-Key weight matrix for the current head\n",
    "        W_QK_i = np.dot(W_Q_i, W_K_i.T) # (d_model, d_model)\n",
    "\n",
    "        # Compute the attention pattern\n",
    "        XWQ = np.einsum('bsd,de->bse', X, W_QK_i) # (batch_size, seq_len, d_model)\n",
    "        A = softmax(np.einsum('bqd,bkd->bqk', XWQ, X) / sqrt_d_head) # (batch_size, seq_len, seq_len)\n",
    "        #print(f\"{A=}\")\n",
    "\n",
    "        # Compute the combined Value-Output weight matrix for the current head\n",
    "        W_VO_i = np.dot(W_V_i, W_O_i) # (d_model, d_model)\n",
    "        #print(f\"{W_VO_i.shape=}\")\n",
    "\n",
    "        # Compute the result for the current head and add it to the overall result\n",
    "        XWV = np.einsum('bsd,de->bse', X, W_VO_i)\n",
    "        #print(f\"{XWV.shape=}\")\n",
    "        \n",
    "        r_i = np.einsum('bqk,bkd->bqd', A, XWV)\n",
    "        # print(f\"{r_i=}\")\n",
    "        # break\n",
    "        result += r_i\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def embedding(x, W_emb):\n",
    "    \"\"\"\n",
    "    Embeds the input sequence using an embedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length)\n",
    "    - W_emb: Embedding matrix of shape (vocab_size, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Embedded input of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    return W_emb[x]\n",
    "\n",
    "def unembedding(x, W_unemb):\n",
    "    \"\"\"\n",
    "    Unembeds the output of the transformer using an unembedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Output matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_unemb: Unembedding matrix of shape (d_model, vocab_size)\n",
    "    \n",
    "    Returns:\n",
    "    - Unembedded output of shape (batch_size, seq_length, vocab_size)\n",
    "    \"\"\"\n",
    "    return np.dot(x, W_unemb)\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 12\n",
    "num_heads = 3\n",
    "d_head = 4\n",
    "vocab_size = 20\n",
    "\n",
    "# Initialize input, embedding matrix, and unembedding matrix\n",
    "X = np.random.randint(vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "# Randomly initialize weight matrices\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "\n",
    "W_emb = np.random.rand(vocab_size, d_model)\n",
    "W_unemb = np.random.rand(d_model, vocab_size)\n",
    "\n",
    "# Embed the input\n",
    "embedded_X = embedding(X, W_emb)\n",
    "\n",
    "# Apply the multi-head attention mechanism\n",
    "attention_output = batched_classic_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads)\n",
    "print(\"Original multi-head attn output shape before unembed:\", attention_output.shape)\n",
    "\n",
    "# Unembed the output\n",
    "final_output = unembedding(attention_output, W_unemb)\n",
    "print(\"After unembed:\", final_output.shape)\n",
    "print(final_output)\n",
    "\n",
    "# Compute the output after refactored multi-head attention\n",
    "refactored_output = batched_refactored_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads, d_head)\n",
    "print(\"Refactored multi-head attn output shape before unembed:\", refactored_output.shape)\n",
    "final_refactored_output = unembedding(refactored_output, W_unemb)\n",
    "\n",
    "print(\"After unembed:\", final_refactored_output.shape)\n",
    "print(final_refactored_output)\n",
    "\n",
    "assert np.allclose(final_output, final_refactored_output), \"The refactored function produces different results!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores[0]=array([[[ 1.06025957e+01, -9.99999982e+08, -9.99999988e+08],\n",
      "        [ 2.29134931e+01,  3.92934814e+01, -9.99999975e+08],\n",
      "        [ 1.25861977e+01,  2.17198193e+01,  1.36278582e+01]],\n",
      "\n",
      "       [[ 1.59389491e+01, -9.99999975e+08, -9.99999986e+08],\n",
      "        [ 2.82133068e+01,  4.45525559e+01, -9.99999974e+08],\n",
      "        [ 1.74981948e+01,  2.80794414e+01,  1.60657226e+01]],\n",
      "\n",
      "       [[ 1.57744620e+01, -9.99999972e+08, -9.99999986e+08],\n",
      "        [ 2.85692821e+01,  5.09878051e+01, -9.99999974e+08],\n",
      "        [ 1.35442650e+01,  2.36867968e+01,  1.19094057e+01]]])\n",
      "Original multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n",
      "A_scores[0]=array([[ 1.06025957e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 2.29134931e+01,  3.92934814e+01, -1.00000000e+09],\n",
      "       [ 1.25861977e+01,  2.17198193e+01,  1.36278582e+01]])\n",
      "A_scores[0]=array([[ 1.59389491e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 2.82133068e+01,  4.45525559e+01, -1.00000000e+09],\n",
      "       [ 1.74981948e+01,  2.80794414e+01,  1.60657226e+01]])\n",
      "A_scores[0]=array([[ 1.57744620e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 2.85692821e+01,  5.09878051e+01, -1.00000000e+09],\n",
      "       [ 1.35442650e+01,  2.36867968e+01,  1.19094057e+01]])\n",
      "Refactored multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Splits the last dimension of x into (num_heads, d_head) for a batch of sequences.\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    x = x.reshape(batch_size, seq_length, num_heads, d_head)\n",
    "    return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, d_head)\n",
    "\n",
    "def batched_classic_multi_head_attention(x, W_Q, W_K, W_V, W_O, num_heads):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention mechanism with weight matrices using np.einsum for a batch of sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Compute Query, Key, Value matrices for each head\n",
    "    Q = split_heads(np.dot(x, W_Q), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    K = split_heads(np.dot(x, W_K), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    V = split_heads(np.dot(x, W_V), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Compute attention scores and weights for each head\n",
    "    attention_scores = np.einsum('bnqd,bnkd->bnqk', Q, K)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "    # Apply autoregressive masking\n",
    "    mask = np.tril(np.ones((seq_length, seq_length)), k=0)\n",
    "    attention_scores = attention_scores - ((1 - mask) * 1e9)\n",
    "    print(f\"{attention_scores[0]=}\")\n",
    "\n",
    "    attention_weights = softmax(attention_scores / sqrt_d_head)\n",
    "    \n",
    "    # Compute weighted sum of value vectors for each head\n",
    "    weighted_sum = np.einsum('bnqk,bnkd->bnqd', attention_weights, V)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Concatenate heads and apply the output weight matrix W_O\n",
    "    weighted_sum = weighted_sum.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def batched_refactored_multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_head):\n",
    "    \"\"\"\n",
    "    Implements refactored multi-head attention mechanism for a batch of sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input matrix of shape (batch_size, seq_len, d_model)\n",
    "    - W_Q, W_K, W_V: Weight matrices for Query, Key, and Value, each of shape (d_model, d_model)\n",
    "    - W_O: Output weight matrix of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    - d_head: Dimension of each head\n",
    "\n",
    "    Returns:\n",
    "    - Output matrix after attention, of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = X.shape\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Initialize the result matrix\n",
    "    result = np.zeros((batch_size, seq_len, d_model))\n",
    "\n",
    "    # Autoregressive mask\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)), k=0).astype(bool)\n",
    "    inverted_mask = np.logical_not(mask)\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        # Extract the weight matrices for the current head\n",
    "        W_Q_i = W_Q[:, i * d_head:(i + 1) * d_head]\n",
    "        W_K_i = W_K[:, i * d_head:(i + 1) * d_head]\n",
    "        W_V_i = W_V[:, i * d_head:(i + 1) * d_head]\n",
    "        W_O_i = W_O[i * d_head:(i + 1) * d_head, :]\n",
    "\n",
    "        # Compute the combined Query-Key weight matrix for the current head\n",
    "        W_QK_i = np.dot(W_Q_i, W_K_i.T) # (d_model, d_model)\n",
    "\n",
    "        # Compute the attention pattern\n",
    "        XWQ = np.einsum('bsd,de->bse', X, W_QK_i) # (batch_size, seq_len, d_model)\n",
    "        A_scores = np.einsum('bqd,bkd->bqk', XWQ, X)\n",
    "        \n",
    "        # Apply autoregressive masking\n",
    "        \n",
    "        A_scores[:, inverted_mask] = -1e9\n",
    "        print(f\"{A_scores[0]=}\")\n",
    "\n",
    "        # compute attention weights\n",
    "        A = softmax(A_scores / sqrt_d_head) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Compute the combined Value-Output weight matrix for the current head\n",
    "        W_VO_i = np.dot(W_V_i, W_O_i) # (d_model, d_model)\n",
    "        #print(f\"{W_VO_i.shape=}\")\n",
    "\n",
    "        # Compute the result for the current head and add it to the overall result\n",
    "        XWV = np.einsum('bsd,de->bse', X, W_VO_i)\n",
    "        #print(f\"{XWV.shape=}\")\n",
    "        \n",
    "        r_i = np.einsum('bqk,bkd->bqd', A, XWV)\n",
    "        # print(f\"{r_i=}\")\n",
    "        # break\n",
    "        result += r_i\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def embedding(x, W_emb):\n",
    "    \"\"\"\n",
    "    Embeds the input sequence using an embedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length)\n",
    "    - W_emb: Embedding matrix of shape (vocab_size, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Embedded input of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    return W_emb[x]\n",
    "\n",
    "def unembedding(x, W_unemb):\n",
    "    \"\"\"\n",
    "    Unembeds the output of the transformer using an unembedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Output matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_unemb: Unembedding matrix of shape (d_model, vocab_size)\n",
    "    \n",
    "    Returns:\n",
    "    - Unembedded output of shape (batch_size, seq_length, vocab_size)\n",
    "    \"\"\"\n",
    "    return np.dot(x, W_unemb)\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 12\n",
    "num_heads = 3\n",
    "d_head = 4\n",
    "vocab_size = 20\n",
    "\n",
    "# Initialize input, embedding matrix, and unembedding matrix\n",
    "X = np.random.randint(vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "# Randomly initialize weight matrices\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "\n",
    "W_emb = np.random.rand(vocab_size, d_model)\n",
    "W_unemb = np.random.rand(d_model, vocab_size)\n",
    "\n",
    "# Embed the input\n",
    "embedded_X = embedding(X, W_emb)\n",
    "\n",
    "# Apply the multi-head attention mechanism\n",
    "attention_output = batched_classic_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads)\n",
    "print(\"Original multi-head attn output shape before unembed:\", attention_output.shape)\n",
    "\n",
    "# Unembed the output\n",
    "final_output = unembedding(attention_output, W_unemb)\n",
    "print(\"After unembed:\", final_output.shape)\n",
    "#print(final_output)\n",
    "\n",
    "# Compute the output after refactored multi-head attention\n",
    "refactored_output = batched_refactored_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads, d_head)\n",
    "print(\"Refactored multi-head attn output shape before unembed:\", refactored_output.shape)\n",
    "final_refactored_output = unembedding(refactored_output, W_unemb)\n",
    "\n",
    "print(\"After unembed:\", final_refactored_output.shape)\n",
    "#print(final_refactored_output)\n",
    "\n",
    "assert np.allclose(final_output, final_refactored_output), \"The refactored function produces different results!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores[0]=array([[[ 3.33760652e+01, -9.99999948e+08, -9.99999976e+08],\n",
      "        [ 5.04926032e+01,  7.87519439e+01, -9.99999964e+08],\n",
      "        [ 2.47624994e+01,  3.86156738e+01,  1.77009859e+01]],\n",
      "\n",
      "       [[ 3.76492073e+01, -9.99999950e+08, -9.99999973e+08],\n",
      "        [ 5.38064569e+01,  7.14070105e+01, -9.99999961e+08],\n",
      "        [ 3.06108894e+01,  4.05976343e+01,  2.22454985e+01]],\n",
      "\n",
      "       [[ 4.13436573e+01, -9.99999947e+08, -9.99999971e+08],\n",
      "        [ 5.80094571e+01,  7.30794967e+01, -9.99999960e+08],\n",
      "        [ 2.82867568e+01,  3.56925183e+01,  1.97417509e+01]]])\n",
      "Original multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n",
      "A_scores[0]=array([[ 3.33760652e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 5.04926032e+01,  7.87519439e+01, -1.00000000e+09],\n",
      "       [ 2.47624994e+01,  3.86156738e+01,  1.77009859e+01]])\n",
      "A_scores[0]=array([[ 3.76492073e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 5.38064569e+01,  7.14070105e+01, -1.00000000e+09],\n",
      "       [ 3.06108894e+01,  4.05976343e+01,  2.22454985e+01]])\n",
      "A_scores[0]=array([[ 4.13436573e+01, -1.00000000e+09, -1.00000000e+09],\n",
      "       [ 5.80094571e+01,  7.30794967e+01, -1.00000000e+09],\n",
      "       [ 2.82867568e+01,  3.56925183e+01,  1.97417509e+01]])\n",
      "Refactored multi-head attn output shape before unembed: (2, 3, 12)\n",
      "After unembed: (2, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Splits the last dimension of x into (num_heads, d_head) for a batch of sequences.\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    x = x.reshape(batch_size, seq_length, num_heads, d_head)\n",
    "    return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, d_head)\n",
    "\n",
    "def batched_classic_multi_head_attention(x, W_Q, W_K, W_V, W_O, num_heads):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention mechanism with weight matrices using np.einsum for a batch of sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, d_model = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Compute Query, Key, Value matrices for each head\n",
    "    Q = split_heads(np.dot(x, W_Q), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    K = split_heads(np.dot(x, W_K), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    V = split_heads(np.dot(x, W_V), num_heads)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Compute attention scores and weights for each head\n",
    "    attention_scores = np.einsum('bnqd,bnkd->bnqk', Q, K)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "    # Apply autoregressive masking\n",
    "    mask = np.tril(np.ones((seq_length, seq_length)), k=0)\n",
    "    attention_scores = attention_scores - ((1 - mask) * 1e9)\n",
    "    print(f\"{attention_scores[0]=}\")\n",
    "\n",
    "    attention_weights = softmax(attention_scores / sqrt_d_head)\n",
    "    \n",
    "    # Compute weighted sum of value vectors for each head\n",
    "    weighted_sum = np.einsum('bnqk,bnkd->bnqd', attention_weights, V)  # (batch_size, num_heads, seq_length, d_head)\n",
    "    \n",
    "    # Concatenate heads and apply the output weight matrix W_O\n",
    "    weighted_sum = weighted_sum.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def batched_refactored_multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_head):\n",
    "    \"\"\"\n",
    "    Implements refactored multi-head attention mechanism for a batch of sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input matrix of shape (batch_size, seq_len, d_model)\n",
    "    - W_Q, W_K, W_V: Weight matrices for Query, Key, and Value, each of shape (d_model, d_model)\n",
    "    - W_O: Output weight matrix of shape (d_model, d_model)\n",
    "    - num_heads: Number of attention heads\n",
    "    - d_head: Dimension of each head\n",
    "\n",
    "    Returns:\n",
    "    - Output matrix after attention, of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = X.shape\n",
    "    sqrt_d_head = np.sqrt(d_head)\n",
    "    \n",
    "    # Initialize the result matrix\n",
    "    result = np.zeros((batch_size, seq_len, d_model))\n",
    "\n",
    "    # Autoregressive mask\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)), k=0).astype(bool)\n",
    "    inverted_mask = np.logical_not(mask)\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        # Extract the weight matrices for the current head\n",
    "        W_Q_i = W_Q[:, i * d_head:(i + 1) * d_head]\n",
    "        W_K_i = W_K[:, i * d_head:(i + 1) * d_head]\n",
    "        W_V_i = W_V[:, i * d_head:(i + 1) * d_head]\n",
    "        W_O_i = W_O[i * d_head:(i + 1) * d_head, :]\n",
    "\n",
    "        # Compute the combined Query-Key weight matrix for the current head\n",
    "        W_QK_i = np.dot(W_Q_i, W_K_i.T) # (d_model, d_model)\n",
    "\n",
    "        # Compute the attention pattern\n",
    "        XWQ = np.einsum('bsd,de->bse', X, W_QK_i) # (batch_size, seq_len, d_model)\n",
    "        A_scores = np.einsum('bqd,bkd->bqk', XWQ, X)\n",
    "        \n",
    "        # Apply autoregressive masking\n",
    "        \n",
    "        A_scores[:, inverted_mask] = -1e9\n",
    "        print(f\"{A_scores[0]=}\")\n",
    "\n",
    "        # compute attention weights\n",
    "        A = softmax(A_scores / sqrt_d_head) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Compute the combined Value-Output weight matrix for the current head\n",
    "        W_VO_i = np.dot(W_V_i, W_O_i) # (d_model, d_model)\n",
    "        #print(f\"{W_VO_i.shape=}\")\n",
    "\n",
    "        # Compute the result for the current head and add it to the overall result\n",
    "        XWV = np.einsum('bsd,de->bse', X, W_VO_i)\n",
    "        #print(f\"{XWV.shape=}\")\n",
    "        \n",
    "        r_i = np.einsum('bqk,bkd->bqd', A, XWV)\n",
    "        # print(f\"{r_i=}\")\n",
    "        # break\n",
    "        result += r_i\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def embedding(x, W_emb):\n",
    "    \"\"\"\n",
    "    Embeds the input sequence using an embedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (batch_size, seq_length)\n",
    "    - W_emb: Embedding matrix of shape (vocab_size, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Embedded input of shape (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    return W_emb[x]\n",
    "\n",
    "def unembedding(x, W_unemb):\n",
    "    \"\"\"\n",
    "    Unembeds the output of the transformer using an unembedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Output matrix of shape (batch_size, seq_length, d_model)\n",
    "    - W_unemb: Unembedding matrix of shape (d_model, vocab_size)\n",
    "    \n",
    "    Returns:\n",
    "    - Unembedded output of shape (batch_size, seq_length, vocab_size)\n",
    "    \"\"\"\n",
    "    return np.dot(x, W_unemb)\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 12\n",
    "num_heads = 3\n",
    "d_head = 4\n",
    "vocab_size = 20\n",
    "\n",
    "# Initialize input, embedding matrix, and unembedding matrix\n",
    "X = np.random.randint(vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "# Randomly initialize weight matrices\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "\n",
    "W_emb = np.random.rand(vocab_size, d_model)\n",
    "W_unemb = np.random.rand(d_model, vocab_size)\n",
    "\n",
    "# Embed the input\n",
    "embedded_X = embedding(X, W_emb)\n",
    "\n",
    "# Apply the multi-head attention mechanism\n",
    "attention_output = batched_classic_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads)\n",
    "print(\"Original multi-head attn output shape before unembed:\", attention_output.shape)\n",
    "\n",
    "# Unembed the output\n",
    "final_output = unembedding(attention_output, W_unemb)\n",
    "print(\"After unembed:\", final_output.shape)\n",
    "#print(final_output)\n",
    "\n",
    "# Compute the output after refactored multi-head attention\n",
    "refactored_output = batched_refactored_multi_head_attention(embedded_X, W_Q, W_K, W_V, W_O, num_heads, d_head)\n",
    "print(\"Refactored multi-head attn output shape before unembed:\", refactored_output.shape)\n",
    "final_refactored_output = unembedding(refactored_output, W_unemb)\n",
    "\n",
    "print(\"After unembed:\", final_refactored_output.shape)\n",
    "#print(final_refactored_output)\n",
    "\n",
    "assert np.allclose(final_output, final_refactored_output), \"The refactored function produces different results!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

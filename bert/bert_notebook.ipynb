{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib\n",
    "\n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../common')\n",
    "\n",
    "import gpt_modules as gpt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import Dropout, LayerNorm, MLP, TransformerConfig, Embedding, GELU\n",
    "from general_modules import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"turn down for what\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1885, 1205, 1111, 1184, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text, add_special_tokens=True, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.query_size = int(hidden_size / num_heads)\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ff = Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, additive_attention_mask: Optional[t.Tensor], num_heads: int):\n",
    "        \"\"\"\n",
    "        Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "        Q: shape (batch, seq, nheads*headsize)\n",
    "        K: shape (batch, seq, nheads*headsize)\n",
    "        V: shape (batch, seq, nheads*headsize)\n",
    "\n",
    "        returns: shape (batch, seq, nheads*headsize)\n",
    "        \"\"\"\n",
    "        Q = rearrange(Q, \"B S (nheads headsize) -> B S nheads headsize\", nheads=num_heads)\n",
    "        K = rearrange(K, \"B S (nheads headsize) -> B S nheads headsize\", nheads=num_heads)\n",
    "        V = rearrange(V, \"B S (nheads headsize) -> B S nheads headsize\", nheads=num_heads)\n",
    "\n",
    "        batch_size, seq_len, nheads, headsize = Q.shape\n",
    "        scores = einsum(\"B Qseq nheads headsize, B Kseq nheads headsize -> B nheads Qseq Kseq\", Q, K)\n",
    "        scores /= Q.shape[-1] ** 0.5\n",
    "\n",
    "        if additive_attention_mask is not None:\n",
    "            attention_scores = attention_scores + additive_attention_mask\n",
    "\n",
    "        scores = t.softmax(scores, dim=-1)\n",
    "        Z = einsum(\"B nheads Qseq Kseq, B Kseq nheads headsize -> B Qseq nheads headsize\", scores, V)\n",
    "        Z = rearrange(Z, \"B Qseq nheads headsize -> B Qseq (nheads headsize)\")\n",
    "        return Z\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor]) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        \"\"\"\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        Z = self.multihead_masked_attention(Q, K, V, additive_attention_mask, self.num_heads)\n",
    "        out = self.ff(Z)\n",
    "        return out \n",
    "\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadMaskedAttention(config.hidden_size, config.num_heads)\n",
    "        self.lnorm1 = LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config.hidden_size, config.dropout)\n",
    "        self.lnorm2 = LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        attn = self.attn(x, additive_attention_mask)\n",
    "        out = self.lnorm1(attn + x)\n",
    "        mlp = self.mlp(out)\n",
    "        out = self.lnorm2(mlp + out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    mask = 1 - one_zero_attention_mask\n",
    "    mask = big_negative_number * mask\n",
    "    return repeat(mask, 'B S -> B 1 1 S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[     0,      0,      0, -10000, -10000, -10000]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_additive_attention_mask(t.tensor([[1,1,1,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tkn_emb = Embedding(2, config.hidden_size)\n",
    "\n",
    "        self.lnorm = LayerNorm(config.hidden_size)\n",
    "        self.dropout = Dropout(p=config.dropout)\n",
    "\n",
    "        decoders = [BERTBlock(config) for l in range(config.num_layers)]\n",
    "        self.blocks = nn.ModuleList(decoders)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        # Embeddings\n",
    "        pos = t.arange(x.shape[1], device=x.device)\n",
    "        if not token_type_ids:\n",
    "            token_type_ids = t.zeros_like(x)\n",
    "\n",
    "        embedding = self.emb(x) + self.pos_emb(pos) + self.tkn_emb(token_type_ids)\n",
    "\n",
    "        # Norm & Dropout\n",
    "        out = self.lnorm(embedding)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Mask\n",
    "        if one_zero_attention_mask:\n",
    "            mask = make_additive_attention_mask(one_zero_attention_mask)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        for b in self.blocks:\n",
    "            out = b(out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.common = BertCommon(config)\n",
    "        self.linear = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.gelu = GELU()\n",
    "        self.lnorm = LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.tied_embed_bias = nn.Parameter(t.zeros(config.vocab_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.common(x)\n",
    "        out = self.gelu(self.linear(out))\n",
    "        out = self.lnorm(out)\n",
    "        out = einsum(\"B S E, V E -> B S V\", out, self.common.emb.weight)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    num_layers = 12,\n",
    "    num_heads = 12,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = 768,\n",
    "    max_seq_len = 512,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1, total params = 108340804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_1</th>\n",
       "      <th>shape_1</th>\n",
       "      <th>num_params_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tied_embed_bias</td>\n",
       "      <td>(28996,)</td>\n",
       "      <td>28996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common.emb.weight</td>\n",
       "      <td>(28996, 768)</td>\n",
       "      <td>22268928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common.pos_emb.weight</td>\n",
       "      <td>(512, 768)</td>\n",
       "      <td>393216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common.tkn_emb.weight</td>\n",
       "      <td>(2, 768)</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common.lnorm.weight</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>common.blocks.11.lnorm2.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>linear.weight</td>\n",
       "      <td>(768, 768)</td>\n",
       "      <td>589824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>linear.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>lnorm.weight</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>lnorm.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name_1       shape_1  num_params_1\n",
       "0                 tied_embed_bias      (28996,)         28996\n",
       "1               common.emb.weight  (28996, 768)      22268928\n",
       "2           common.pos_emb.weight    (512, 768)        393216\n",
       "3           common.tkn_emb.weight      (2, 768)          1536\n",
       "4             common.lnorm.weight        (768,)           768\n",
       "..                            ...           ...           ...\n",
       "197  common.blocks.11.lnorm2.bias        (768,)           768\n",
       "198                 linear.weight    (768, 768)        589824\n",
       "199                   linear.bias        (768,)           768\n",
       "200                  lnorm.weight        (768,)           768\n",
       "201                    lnorm.bias        (768,)           768\n",
       "\n",
       "[202 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2, total params = 108340804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_params_2</th>\n",
       "      <th>shape_2</th>\n",
       "      <th>name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22268928</td>\n",
       "      <td>(28996, 768)</td>\n",
       "      <td>bert.embeddings.word_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393216</td>\n",
       "      <td>(512, 768)</td>\n",
       "      <td>bert.embeddings.position_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1536</td>\n",
       "      <td>(2, 768)</td>\n",
       "      <td>bert.embeddings.token_type_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>bert.embeddings.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>bert.embeddings.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>28996</td>\n",
       "      <td>(28996,)</td>\n",
       "      <td>cls.predictions.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>589824</td>\n",
       "      <td>(768, 768)</td>\n",
       "      <td>cls.predictions.transform.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>768</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>cls.predictions.transform.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>768</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>cls.predictions.transform.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>768</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>cls.predictions.transform.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_params_2       shape_2                                        name_2\n",
       "0        22268928  (28996, 768)        bert.embeddings.word_embeddings.weight\n",
       "1          393216    (512, 768)    bert.embeddings.position_embeddings.weight\n",
       "2            1536      (2, 768)  bert.embeddings.token_type_embeddings.weight\n",
       "3             768        (768,)              bert.embeddings.LayerNorm.weight\n",
       "4             768        (768,)                bert.embeddings.LayerNorm.bias\n",
       "..            ...           ...                                           ...\n",
       "197         28996      (28996,)                          cls.predictions.bias\n",
       "198        589824    (768, 768)        cls.predictions.transform.dense.weight\n",
       "199           768        (768,)          cls.predictions.transform.dense.bias\n",
       "200           768        (768,)    cls.predictions.transform.LayerNorm.weight\n",
       "201           768        (768,)      cls.predictions.transform.LayerNorm.bias\n",
       "\n",
       "[202 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter counts don't match up exactly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f4ec0_row0_col2, #T_f4ec0_row197_col3 {\n",
       "  background-color: #2e6d8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f4ec0_row0_col3, #T_f4ec0_row1_col2 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f4ec0_row1_col3, #T_f4ec0_row2_col2 {\n",
       "  background-color: #24aa83;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f4ec0_row2_col3, #T_f4ec0_row3_col2 {\n",
       "  background-color: #481a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f4ec0_row3_col3, #T_f4ec0_row4_col2, #T_f4ec0_row4_col3, #T_f4ec0_row5_col2, #T_f4ec0_row6_col3, #T_f4ec0_row7_col2, #T_f4ec0_row8_col3, #T_f4ec0_row9_col2, #T_f4ec0_row10_col3, #T_f4ec0_row11_col2, #T_f4ec0_row12_col3, #T_f4ec0_row13_col2, #T_f4ec0_row13_col3, #T_f4ec0_row14_col2, #T_f4ec0_row14_col3, #T_f4ec0_row15_col2, #T_f4ec0_row18_col3, #T_f4ec0_row19_col2, #T_f4ec0_row19_col3, #T_f4ec0_row20_col2, #T_f4ec0_row20_col3, #T_f4ec0_row21_col2, #T_f4ec0_row22_col3, #T_f4ec0_row23_col2, #T_f4ec0_row24_col3, #T_f4ec0_row25_col2, #T_f4ec0_row26_col3, #T_f4ec0_row27_col2, #T_f4ec0_row28_col3, #T_f4ec0_row29_col2, #T_f4ec0_row29_col3, #T_f4ec0_row30_col2, #T_f4ec0_row30_col3, #T_f4ec0_row31_col2, #T_f4ec0_row34_col3, #T_f4ec0_row35_col2, #T_f4ec0_row35_col3, #T_f4ec0_row36_col2, #T_f4ec0_row36_col3, #T_f4ec0_row37_col2, #T_f4ec0_row38_col3, #T_f4ec0_row39_col2, #T_f4ec0_row40_col3, #T_f4ec0_row41_col2, #T_f4ec0_row42_col3, #T_f4ec0_row43_col2, #T_f4ec0_row44_col3, #T_f4ec0_row45_col2, #T_f4ec0_row45_col3, #T_f4ec0_row46_col2, #T_f4ec0_row46_col3, #T_f4ec0_row47_col2, #T_f4ec0_row50_col3, #T_f4ec0_row51_col2, #T_f4ec0_row51_col3, #T_f4ec0_row52_col2, #T_f4ec0_row52_col3, #T_f4ec0_row53_col2, #T_f4ec0_row54_col3, #T_f4ec0_row55_col2, #T_f4ec0_row56_col3, #T_f4ec0_row57_col2, #T_f4ec0_row58_col3, #T_f4ec0_row59_col2, #T_f4ec0_row60_col3, #T_f4ec0_row61_col2, #T_f4ec0_row61_col3, #T_f4ec0_row62_col2, #T_f4ec0_row62_col3, #T_f4ec0_row63_col2, #T_f4ec0_row66_col3, #T_f4ec0_row67_col2, #T_f4ec0_row67_col3, #T_f4ec0_row68_col2, #T_f4ec0_row68_col3, #T_f4ec0_row69_col2, #T_f4ec0_row70_col3, #T_f4ec0_row71_col2, #T_f4ec0_row72_col3, #T_f4ec0_row73_col2, #T_f4ec0_row74_col3, #T_f4ec0_row75_col2, #T_f4ec0_row76_col3, #T_f4ec0_row77_col2, #T_f4ec0_row77_col3, #T_f4ec0_row78_col2, #T_f4ec0_row78_col3, #T_f4ec0_row79_col2, #T_f4ec0_row82_col3, #T_f4ec0_row83_col2, #T_f4ec0_row83_col3, #T_f4ec0_row84_col2, #T_f4ec0_row84_col3, #T_f4ec0_row85_col2, #T_f4ec0_row86_col3, #T_f4ec0_row87_col2, #T_f4ec0_row88_col3, #T_f4ec0_row89_col2, #T_f4ec0_row90_col3, #T_f4ec0_row91_col2, #T_f4ec0_row92_col3, #T_f4ec0_row93_col2, #T_f4ec0_row93_col3, #T_f4ec0_row94_col2, #T_f4ec0_row94_col3, #T_f4ec0_row95_col2, #T_f4ec0_row98_col3, #T_f4ec0_row99_col2, #T_f4ec0_row99_col3, #T_f4ec0_row100_col2, #T_f4ec0_row100_col3, #T_f4ec0_row101_col2, #T_f4ec0_row102_col3, #T_f4ec0_row103_col2, #T_f4ec0_row104_col3, #T_f4ec0_row105_col2, #T_f4ec0_row106_col3, #T_f4ec0_row107_col2, #T_f4ec0_row108_col3, #T_f4ec0_row109_col2, #T_f4ec0_row109_col3, #T_f4ec0_row110_col2, #T_f4ec0_row110_col3, #T_f4ec0_row111_col2, #T_f4ec0_row114_col3, #T_f4ec0_row115_col2, #T_f4ec0_row115_col3, #T_f4ec0_row116_col2, #T_f4ec0_row116_col3, #T_f4ec0_row117_col2, #T_f4ec0_row118_col3, #T_f4ec0_row119_col2, #T_f4ec0_row120_col3, #T_f4ec0_row121_col2, #T_f4ec0_row122_col3, #T_f4ec0_row123_col2, #T_f4ec0_row124_col3, #T_f4ec0_row125_col2, #T_f4ec0_row125_col3, #T_f4ec0_row126_col2, #T_f4ec0_row126_col3, #T_f4ec0_row127_col2, #T_f4ec0_row130_col3, #T_f4ec0_row131_col2, #T_f4ec0_row131_col3, #T_f4ec0_row132_col2, #T_f4ec0_row132_col3, #T_f4ec0_row133_col2, #T_f4ec0_row134_col3, #T_f4ec0_row135_col2, #T_f4ec0_row136_col3, #T_f4ec0_row137_col2, #T_f4ec0_row138_col3, #T_f4ec0_row139_col2, #T_f4ec0_row140_col3, #T_f4ec0_row141_col2, #T_f4ec0_row141_col3, #T_f4ec0_row142_col2, #T_f4ec0_row142_col3, #T_f4ec0_row143_col2, #T_f4ec0_row146_col3, #T_f4ec0_row147_col2, #T_f4ec0_row147_col3, #T_f4ec0_row148_col2, #T_f4ec0_row148_col3, #T_f4ec0_row149_col2, #T_f4ec0_row150_col3, #T_f4ec0_row151_col2, #T_f4ec0_row152_col3, #T_f4ec0_row153_col2, #T_f4ec0_row154_col3, #T_f4ec0_row155_col2, #T_f4ec0_row156_col3, #T_f4ec0_row157_col2, #T_f4ec0_row157_col3, #T_f4ec0_row158_col2, #T_f4ec0_row158_col3, #T_f4ec0_row159_col2, #T_f4ec0_row162_col3, #T_f4ec0_row163_col2, #T_f4ec0_row163_col3, #T_f4ec0_row164_col2, #T_f4ec0_row164_col3, #T_f4ec0_row165_col2, #T_f4ec0_row166_col3, #T_f4ec0_row167_col2, #T_f4ec0_row168_col3, #T_f4ec0_row169_col2, #T_f4ec0_row170_col3, #T_f4ec0_row171_col2, #T_f4ec0_row172_col3, #T_f4ec0_row173_col2, #T_f4ec0_row173_col3, #T_f4ec0_row174_col2, #T_f4ec0_row174_col3, #T_f4ec0_row175_col2, #T_f4ec0_row178_col3, #T_f4ec0_row179_col2, #T_f4ec0_row179_col3, #T_f4ec0_row180_col2, #T_f4ec0_row180_col3, #T_f4ec0_row181_col2, #T_f4ec0_row182_col3, #T_f4ec0_row183_col2, #T_f4ec0_row184_col3, #T_f4ec0_row185_col2, #T_f4ec0_row186_col3, #T_f4ec0_row187_col2, #T_f4ec0_row188_col3, #T_f4ec0_row189_col2, #T_f4ec0_row189_col3, #T_f4ec0_row190_col2, #T_f4ec0_row190_col3, #T_f4ec0_row191_col2, #T_f4ec0_row194_col3, #T_f4ec0_row195_col2, #T_f4ec0_row195_col3, #T_f4ec0_row196_col2, #T_f4ec0_row196_col3, #T_f4ec0_row197_col2, #T_f4ec0_row199_col2, #T_f4ec0_row199_col3, #T_f4ec0_row200_col2, #T_f4ec0_row200_col3, #T_f4ec0_row201_col2, #T_f4ec0_row201_col3 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f4ec0_row5_col3, #T_f4ec0_row6_col2, #T_f4ec0_row7_col3, #T_f4ec0_row8_col2, #T_f4ec0_row9_col3, #T_f4ec0_row10_col2, #T_f4ec0_row11_col3, #T_f4ec0_row12_col2, #T_f4ec0_row21_col3, #T_f4ec0_row22_col2, #T_f4ec0_row23_col3, #T_f4ec0_row24_col2, #T_f4ec0_row25_col3, #T_f4ec0_row26_col2, #T_f4ec0_row27_col3, #T_f4ec0_row28_col2, #T_f4ec0_row37_col3, #T_f4ec0_row38_col2, #T_f4ec0_row39_col3, #T_f4ec0_row40_col2, #T_f4ec0_row41_col3, #T_f4ec0_row42_col2, #T_f4ec0_row43_col3, #T_f4ec0_row44_col2, #T_f4ec0_row53_col3, #T_f4ec0_row54_col2, #T_f4ec0_row55_col3, #T_f4ec0_row56_col2, #T_f4ec0_row57_col3, #T_f4ec0_row58_col2, #T_f4ec0_row59_col3, #T_f4ec0_row60_col2, #T_f4ec0_row69_col3, #T_f4ec0_row70_col2, #T_f4ec0_row71_col3, #T_f4ec0_row72_col2, #T_f4ec0_row73_col3, #T_f4ec0_row74_col2, #T_f4ec0_row75_col3, #T_f4ec0_row76_col2, #T_f4ec0_row85_col3, #T_f4ec0_row86_col2, #T_f4ec0_row87_col3, #T_f4ec0_row88_col2, #T_f4ec0_row89_col3, #T_f4ec0_row90_col2, #T_f4ec0_row91_col3, #T_f4ec0_row92_col2, #T_f4ec0_row101_col3, #T_f4ec0_row102_col2, #T_f4ec0_row103_col3, #T_f4ec0_row104_col2, #T_f4ec0_row105_col3, #T_f4ec0_row106_col2, #T_f4ec0_row107_col3, #T_f4ec0_row108_col2, #T_f4ec0_row117_col3, #T_f4ec0_row118_col2, #T_f4ec0_row119_col3, #T_f4ec0_row120_col2, #T_f4ec0_row121_col3, #T_f4ec0_row122_col2, #T_f4ec0_row123_col3, #T_f4ec0_row124_col2, #T_f4ec0_row133_col3, #T_f4ec0_row134_col2, #T_f4ec0_row135_col3, #T_f4ec0_row136_col2, #T_f4ec0_row137_col3, #T_f4ec0_row138_col2, #T_f4ec0_row139_col3, #T_f4ec0_row140_col2, #T_f4ec0_row149_col3, #T_f4ec0_row150_col2, #T_f4ec0_row151_col3, #T_f4ec0_row152_col2, #T_f4ec0_row153_col3, #T_f4ec0_row154_col2, #T_f4ec0_row155_col3, #T_f4ec0_row156_col2, #T_f4ec0_row165_col3, #T_f4ec0_row166_col2, #T_f4ec0_row167_col3, #T_f4ec0_row168_col2, #T_f4ec0_row169_col3, #T_f4ec0_row170_col2, #T_f4ec0_row171_col3, #T_f4ec0_row172_col2, #T_f4ec0_row181_col3, #T_f4ec0_row182_col2, #T_f4ec0_row183_col3, #T_f4ec0_row184_col2, #T_f4ec0_row185_col3, #T_f4ec0_row186_col2, #T_f4ec0_row187_col3, #T_f4ec0_row188_col2, #T_f4ec0_row198_col2, #T_f4ec0_row198_col3 {\n",
       "  background-color: #2eb37c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f4ec0_row15_col3, #T_f4ec0_row16_col2, #T_f4ec0_row17_col3, #T_f4ec0_row18_col2, #T_f4ec0_row31_col3, #T_f4ec0_row32_col2, #T_f4ec0_row33_col3, #T_f4ec0_row34_col2, #T_f4ec0_row47_col3, #T_f4ec0_row48_col2, #T_f4ec0_row49_col3, #T_f4ec0_row50_col2, #T_f4ec0_row63_col3, #T_f4ec0_row64_col2, #T_f4ec0_row65_col3, #T_f4ec0_row66_col2, #T_f4ec0_row79_col3, #T_f4ec0_row80_col2, #T_f4ec0_row81_col3, #T_f4ec0_row82_col2, #T_f4ec0_row95_col3, #T_f4ec0_row96_col2, #T_f4ec0_row97_col3, #T_f4ec0_row98_col2, #T_f4ec0_row111_col3, #T_f4ec0_row112_col2, #T_f4ec0_row113_col3, #T_f4ec0_row114_col2, #T_f4ec0_row127_col3, #T_f4ec0_row128_col2, #T_f4ec0_row129_col3, #T_f4ec0_row130_col2, #T_f4ec0_row143_col3, #T_f4ec0_row144_col2, #T_f4ec0_row145_col3, #T_f4ec0_row146_col2, #T_f4ec0_row159_col3, #T_f4ec0_row160_col2, #T_f4ec0_row161_col3, #T_f4ec0_row162_col2, #T_f4ec0_row175_col3, #T_f4ec0_row176_col2, #T_f4ec0_row177_col3, #T_f4ec0_row178_col2, #T_f4ec0_row191_col3, #T_f4ec0_row192_col2, #T_f4ec0_row193_col3, #T_f4ec0_row194_col2 {\n",
       "  background-color: #70cf57;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f4ec0_row16_col3, #T_f4ec0_row17_col2, #T_f4ec0_row32_col3, #T_f4ec0_row33_col2, #T_f4ec0_row48_col3, #T_f4ec0_row49_col2, #T_f4ec0_row64_col3, #T_f4ec0_row65_col2, #T_f4ec0_row80_col3, #T_f4ec0_row81_col2, #T_f4ec0_row96_col3, #T_f4ec0_row97_col2, #T_f4ec0_row112_col3, #T_f4ec0_row113_col2, #T_f4ec0_row128_col3, #T_f4ec0_row129_col2, #T_f4ec0_row144_col3, #T_f4ec0_row145_col2, #T_f4ec0_row160_col3, #T_f4ec0_row161_col2, #T_f4ec0_row176_col3, #T_f4ec0_row177_col2, #T_f4ec0_row192_col3, #T_f4ec0_row193_col2 {\n",
       "  background-color: #472f7d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f4ec0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f4ec0_level0_col0\" class=\"col_heading level0 col0\" >name_1</th>\n",
       "      <th id=\"T_f4ec0_level0_col1\" class=\"col_heading level0 col1\" >shape_1</th>\n",
       "      <th id=\"T_f4ec0_level0_col2\" class=\"col_heading level0 col2\" >num_params_1</th>\n",
       "      <th id=\"T_f4ec0_level0_col3\" class=\"col_heading level0 col3\" >num_params_2</th>\n",
       "      <th id=\"T_f4ec0_level0_col4\" class=\"col_heading level0 col4\" >shape_2</th>\n",
       "      <th id=\"T_f4ec0_level0_col5\" class=\"col_heading level0 col5\" >name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f4ec0_row0_col0\" class=\"data row0 col0\" >tied_embed_bias</td>\n",
       "      <td id=\"T_f4ec0_row0_col1\" class=\"data row0 col1\" >(28996,)</td>\n",
       "      <td id=\"T_f4ec0_row0_col2\" class=\"data row0 col2\" >28996</td>\n",
       "      <td id=\"T_f4ec0_row0_col3\" class=\"data row0 col3\" >22268928</td>\n",
       "      <td id=\"T_f4ec0_row0_col4\" class=\"data row0 col4\" >(28996, 768)</td>\n",
       "      <td id=\"T_f4ec0_row0_col5\" class=\"data row0 col5\" >bert.embeddings.word_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f4ec0_row1_col0\" class=\"data row1 col0\" >common.emb.weight</td>\n",
       "      <td id=\"T_f4ec0_row1_col1\" class=\"data row1 col1\" >(28996, 768)</td>\n",
       "      <td id=\"T_f4ec0_row1_col2\" class=\"data row1 col2\" >22268928</td>\n",
       "      <td id=\"T_f4ec0_row1_col3\" class=\"data row1 col3\" >393216</td>\n",
       "      <td id=\"T_f4ec0_row1_col4\" class=\"data row1 col4\" >(512, 768)</td>\n",
       "      <td id=\"T_f4ec0_row1_col5\" class=\"data row1 col5\" >bert.embeddings.position_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f4ec0_row2_col0\" class=\"data row2 col0\" >common.pos_emb.weight</td>\n",
       "      <td id=\"T_f4ec0_row2_col1\" class=\"data row2 col1\" >(512, 768)</td>\n",
       "      <td id=\"T_f4ec0_row2_col2\" class=\"data row2 col2\" >393216</td>\n",
       "      <td id=\"T_f4ec0_row2_col3\" class=\"data row2 col3\" >1536</td>\n",
       "      <td id=\"T_f4ec0_row2_col4\" class=\"data row2 col4\" >(2, 768)</td>\n",
       "      <td id=\"T_f4ec0_row2_col5\" class=\"data row2 col5\" >bert.embeddings.token_type_embeddings.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f4ec0_row3_col0\" class=\"data row3 col0\" >common.tkn_emb.weight</td>\n",
       "      <td id=\"T_f4ec0_row3_col1\" class=\"data row3 col1\" >(2, 768)</td>\n",
       "      <td id=\"T_f4ec0_row3_col2\" class=\"data row3 col2\" >1536</td>\n",
       "      <td id=\"T_f4ec0_row3_col3\" class=\"data row3 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row3_col4\" class=\"data row3 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row3_col5\" class=\"data row3 col5\" >bert.embeddings.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f4ec0_row4_col0\" class=\"data row4 col0\" >common.lnorm.weight</td>\n",
       "      <td id=\"T_f4ec0_row4_col1\" class=\"data row4 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row4_col2\" class=\"data row4 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row4_col3\" class=\"data row4 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row4_col4\" class=\"data row4 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row4_col5\" class=\"data row4 col5\" >bert.embeddings.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f4ec0_row5_col0\" class=\"data row5 col0\" >common.lnorm.bias</td>\n",
       "      <td id=\"T_f4ec0_row5_col1\" class=\"data row5 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row5_col2\" class=\"data row5 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row5_col3\" class=\"data row5 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row5_col4\" class=\"data row5 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row5_col5\" class=\"data row5 col5\" >bert.encoder.layer.0.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f4ec0_row6_col0\" class=\"data row6 col0\" >common.blocks.0.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row6_col1\" class=\"data row6 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row6_col2\" class=\"data row6 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row6_col3\" class=\"data row6 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row6_col4\" class=\"data row6 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row6_col5\" class=\"data row6 col5\" >bert.encoder.layer.0.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f4ec0_row7_col0\" class=\"data row7 col0\" >common.blocks.0.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row7_col1\" class=\"data row7 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row7_col2\" class=\"data row7 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row7_col3\" class=\"data row7 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row7_col4\" class=\"data row7 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row7_col5\" class=\"data row7 col5\" >bert.encoder.layer.0.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f4ec0_row8_col0\" class=\"data row8 col0\" >common.blocks.0.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row8_col1\" class=\"data row8 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row8_col2\" class=\"data row8 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row8_col3\" class=\"data row8 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row8_col4\" class=\"data row8 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row8_col5\" class=\"data row8 col5\" >bert.encoder.layer.0.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f4ec0_row9_col0\" class=\"data row9 col0\" >common.blocks.0.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row9_col1\" class=\"data row9 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row9_col2\" class=\"data row9 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row9_col3\" class=\"data row9 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row9_col4\" class=\"data row9 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row9_col5\" class=\"data row9 col5\" >bert.encoder.layer.0.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f4ec0_row10_col0\" class=\"data row10 col0\" >common.blocks.0.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row10_col1\" class=\"data row10 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row10_col2\" class=\"data row10 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row10_col3\" class=\"data row10 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row10_col4\" class=\"data row10 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row10_col5\" class=\"data row10 col5\" >bert.encoder.layer.0.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f4ec0_row11_col0\" class=\"data row11 col0\" >common.blocks.0.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row11_col1\" class=\"data row11 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row11_col2\" class=\"data row11 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row11_col3\" class=\"data row11 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row11_col4\" class=\"data row11 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row11_col5\" class=\"data row11 col5\" >bert.encoder.layer.0.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f4ec0_row12_col0\" class=\"data row12 col0\" >common.blocks.0.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row12_col1\" class=\"data row12 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row12_col2\" class=\"data row12 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row12_col3\" class=\"data row12 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row12_col4\" class=\"data row12 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row12_col5\" class=\"data row12 col5\" >bert.encoder.layer.0.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f4ec0_row13_col0\" class=\"data row13 col0\" >common.blocks.0.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row13_col1\" class=\"data row13 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row13_col2\" class=\"data row13 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row13_col3\" class=\"data row13 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row13_col4\" class=\"data row13 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row13_col5\" class=\"data row13 col5\" >bert.encoder.layer.0.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f4ec0_row14_col0\" class=\"data row14 col0\" >common.blocks.0.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row14_col1\" class=\"data row14 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row14_col2\" class=\"data row14 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row14_col3\" class=\"data row14 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row14_col4\" class=\"data row14 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row14_col5\" class=\"data row14 col5\" >bert.encoder.layer.0.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f4ec0_row15_col0\" class=\"data row15 col0\" >common.blocks.0.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row15_col1\" class=\"data row15 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row15_col2\" class=\"data row15 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row15_col3\" class=\"data row15 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row15_col4\" class=\"data row15 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row15_col5\" class=\"data row15 col5\" >bert.encoder.layer.0.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f4ec0_row16_col0\" class=\"data row16 col0\" >common.blocks.0.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row16_col1\" class=\"data row16 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row16_col2\" class=\"data row16 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row16_col3\" class=\"data row16 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row16_col4\" class=\"data row16 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row16_col5\" class=\"data row16 col5\" >bert.encoder.layer.0.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f4ec0_row17_col0\" class=\"data row17 col0\" >common.blocks.0.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row17_col1\" class=\"data row17 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row17_col2\" class=\"data row17 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row17_col3\" class=\"data row17 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row17_col4\" class=\"data row17 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row17_col5\" class=\"data row17 col5\" >bert.encoder.layer.0.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f4ec0_row18_col0\" class=\"data row18 col0\" >common.blocks.0.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row18_col1\" class=\"data row18 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row18_col2\" class=\"data row18 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row18_col3\" class=\"data row18 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row18_col4\" class=\"data row18 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row18_col5\" class=\"data row18 col5\" >bert.encoder.layer.0.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f4ec0_row19_col0\" class=\"data row19 col0\" >common.blocks.0.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row19_col1\" class=\"data row19 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row19_col2\" class=\"data row19 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row19_col3\" class=\"data row19 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row19_col4\" class=\"data row19 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row19_col5\" class=\"data row19 col5\" >bert.encoder.layer.0.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_f4ec0_row20_col0\" class=\"data row20 col0\" >common.blocks.0.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row20_col1\" class=\"data row20 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row20_col2\" class=\"data row20 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row20_col3\" class=\"data row20 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row20_col4\" class=\"data row20 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row20_col5\" class=\"data row20 col5\" >bert.encoder.layer.0.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_f4ec0_row21_col0\" class=\"data row21 col0\" >common.blocks.0.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row21_col1\" class=\"data row21 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row21_col2\" class=\"data row21 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row21_col3\" class=\"data row21 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row21_col4\" class=\"data row21 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row21_col5\" class=\"data row21 col5\" >bert.encoder.layer.1.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_f4ec0_row22_col0\" class=\"data row22 col0\" >common.blocks.1.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row22_col1\" class=\"data row22 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row22_col2\" class=\"data row22 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row22_col3\" class=\"data row22 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row22_col4\" class=\"data row22 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row22_col5\" class=\"data row22 col5\" >bert.encoder.layer.1.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_f4ec0_row23_col0\" class=\"data row23 col0\" >common.blocks.1.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row23_col1\" class=\"data row23 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row23_col2\" class=\"data row23 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row23_col3\" class=\"data row23 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row23_col4\" class=\"data row23 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row23_col5\" class=\"data row23 col5\" >bert.encoder.layer.1.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_f4ec0_row24_col0\" class=\"data row24 col0\" >common.blocks.1.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row24_col1\" class=\"data row24 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row24_col2\" class=\"data row24 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row24_col3\" class=\"data row24 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row24_col4\" class=\"data row24 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row24_col5\" class=\"data row24 col5\" >bert.encoder.layer.1.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_f4ec0_row25_col0\" class=\"data row25 col0\" >common.blocks.1.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row25_col1\" class=\"data row25 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row25_col2\" class=\"data row25 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row25_col3\" class=\"data row25 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row25_col4\" class=\"data row25 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row25_col5\" class=\"data row25 col5\" >bert.encoder.layer.1.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_f4ec0_row26_col0\" class=\"data row26 col0\" >common.blocks.1.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row26_col1\" class=\"data row26 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row26_col2\" class=\"data row26 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row26_col3\" class=\"data row26 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row26_col4\" class=\"data row26 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row26_col5\" class=\"data row26 col5\" >bert.encoder.layer.1.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_f4ec0_row27_col0\" class=\"data row27 col0\" >common.blocks.1.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row27_col1\" class=\"data row27 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row27_col2\" class=\"data row27 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row27_col3\" class=\"data row27 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row27_col4\" class=\"data row27 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row27_col5\" class=\"data row27 col5\" >bert.encoder.layer.1.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_f4ec0_row28_col0\" class=\"data row28 col0\" >common.blocks.1.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row28_col1\" class=\"data row28 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row28_col2\" class=\"data row28 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row28_col3\" class=\"data row28 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row28_col4\" class=\"data row28 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row28_col5\" class=\"data row28 col5\" >bert.encoder.layer.1.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_f4ec0_row29_col0\" class=\"data row29 col0\" >common.blocks.1.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row29_col1\" class=\"data row29 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row29_col2\" class=\"data row29 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row29_col3\" class=\"data row29 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row29_col4\" class=\"data row29 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row29_col5\" class=\"data row29 col5\" >bert.encoder.layer.1.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_f4ec0_row30_col0\" class=\"data row30 col0\" >common.blocks.1.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row30_col1\" class=\"data row30 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row30_col2\" class=\"data row30 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row30_col3\" class=\"data row30 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row30_col4\" class=\"data row30 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row30_col5\" class=\"data row30 col5\" >bert.encoder.layer.1.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_f4ec0_row31_col0\" class=\"data row31 col0\" >common.blocks.1.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row31_col1\" class=\"data row31 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row31_col2\" class=\"data row31 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row31_col3\" class=\"data row31 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row31_col4\" class=\"data row31 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row31_col5\" class=\"data row31 col5\" >bert.encoder.layer.1.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_f4ec0_row32_col0\" class=\"data row32 col0\" >common.blocks.1.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row32_col1\" class=\"data row32 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row32_col2\" class=\"data row32 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row32_col3\" class=\"data row32 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row32_col4\" class=\"data row32 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row32_col5\" class=\"data row32 col5\" >bert.encoder.layer.1.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_f4ec0_row33_col0\" class=\"data row33 col0\" >common.blocks.1.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row33_col1\" class=\"data row33 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row33_col2\" class=\"data row33 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row33_col3\" class=\"data row33 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row33_col4\" class=\"data row33 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row33_col5\" class=\"data row33 col5\" >bert.encoder.layer.1.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_f4ec0_row34_col0\" class=\"data row34 col0\" >common.blocks.1.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row34_col1\" class=\"data row34 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row34_col2\" class=\"data row34 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row34_col3\" class=\"data row34 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row34_col4\" class=\"data row34 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row34_col5\" class=\"data row34 col5\" >bert.encoder.layer.1.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_f4ec0_row35_col0\" class=\"data row35 col0\" >common.blocks.1.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row35_col1\" class=\"data row35 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row35_col2\" class=\"data row35 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row35_col3\" class=\"data row35 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row35_col4\" class=\"data row35 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row35_col5\" class=\"data row35 col5\" >bert.encoder.layer.1.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_f4ec0_row36_col0\" class=\"data row36 col0\" >common.blocks.1.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row36_col1\" class=\"data row36 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row36_col2\" class=\"data row36 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row36_col3\" class=\"data row36 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row36_col4\" class=\"data row36 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row36_col5\" class=\"data row36 col5\" >bert.encoder.layer.1.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_f4ec0_row37_col0\" class=\"data row37 col0\" >common.blocks.1.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row37_col1\" class=\"data row37 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row37_col2\" class=\"data row37 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row37_col3\" class=\"data row37 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row37_col4\" class=\"data row37 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row37_col5\" class=\"data row37 col5\" >bert.encoder.layer.2.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_f4ec0_row38_col0\" class=\"data row38 col0\" >common.blocks.2.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row38_col1\" class=\"data row38 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row38_col2\" class=\"data row38 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row38_col3\" class=\"data row38 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row38_col4\" class=\"data row38 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row38_col5\" class=\"data row38 col5\" >bert.encoder.layer.2.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_f4ec0_row39_col0\" class=\"data row39 col0\" >common.blocks.2.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row39_col1\" class=\"data row39 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row39_col2\" class=\"data row39 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row39_col3\" class=\"data row39 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row39_col4\" class=\"data row39 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row39_col5\" class=\"data row39 col5\" >bert.encoder.layer.2.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_f4ec0_row40_col0\" class=\"data row40 col0\" >common.blocks.2.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row40_col1\" class=\"data row40 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row40_col2\" class=\"data row40 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row40_col3\" class=\"data row40 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row40_col4\" class=\"data row40 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row40_col5\" class=\"data row40 col5\" >bert.encoder.layer.2.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_f4ec0_row41_col0\" class=\"data row41 col0\" >common.blocks.2.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row41_col1\" class=\"data row41 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row41_col2\" class=\"data row41 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row41_col3\" class=\"data row41 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row41_col4\" class=\"data row41 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row41_col5\" class=\"data row41 col5\" >bert.encoder.layer.2.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_f4ec0_row42_col0\" class=\"data row42 col0\" >common.blocks.2.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row42_col1\" class=\"data row42 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row42_col2\" class=\"data row42 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row42_col3\" class=\"data row42 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row42_col4\" class=\"data row42 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row42_col5\" class=\"data row42 col5\" >bert.encoder.layer.2.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_f4ec0_row43_col0\" class=\"data row43 col0\" >common.blocks.2.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row43_col1\" class=\"data row43 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row43_col2\" class=\"data row43 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row43_col3\" class=\"data row43 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row43_col4\" class=\"data row43 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row43_col5\" class=\"data row43 col5\" >bert.encoder.layer.2.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_f4ec0_row44_col0\" class=\"data row44 col0\" >common.blocks.2.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row44_col1\" class=\"data row44 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row44_col2\" class=\"data row44 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row44_col3\" class=\"data row44 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row44_col4\" class=\"data row44 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row44_col5\" class=\"data row44 col5\" >bert.encoder.layer.2.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_f4ec0_row45_col0\" class=\"data row45 col0\" >common.blocks.2.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row45_col1\" class=\"data row45 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row45_col2\" class=\"data row45 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row45_col3\" class=\"data row45 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row45_col4\" class=\"data row45 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row45_col5\" class=\"data row45 col5\" >bert.encoder.layer.2.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_f4ec0_row46_col0\" class=\"data row46 col0\" >common.blocks.2.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row46_col1\" class=\"data row46 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row46_col2\" class=\"data row46 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row46_col3\" class=\"data row46 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row46_col4\" class=\"data row46 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row46_col5\" class=\"data row46 col5\" >bert.encoder.layer.2.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_f4ec0_row47_col0\" class=\"data row47 col0\" >common.blocks.2.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row47_col1\" class=\"data row47 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row47_col2\" class=\"data row47 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row47_col3\" class=\"data row47 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row47_col4\" class=\"data row47 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row47_col5\" class=\"data row47 col5\" >bert.encoder.layer.2.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_f4ec0_row48_col0\" class=\"data row48 col0\" >common.blocks.2.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row48_col1\" class=\"data row48 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row48_col2\" class=\"data row48 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row48_col3\" class=\"data row48 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row48_col4\" class=\"data row48 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row48_col5\" class=\"data row48 col5\" >bert.encoder.layer.2.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_f4ec0_row49_col0\" class=\"data row49 col0\" >common.blocks.2.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row49_col1\" class=\"data row49 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row49_col2\" class=\"data row49 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row49_col3\" class=\"data row49 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row49_col4\" class=\"data row49 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row49_col5\" class=\"data row49 col5\" >bert.encoder.layer.2.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "      <td id=\"T_f4ec0_row50_col0\" class=\"data row50 col0\" >common.blocks.2.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row50_col1\" class=\"data row50 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row50_col2\" class=\"data row50 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row50_col3\" class=\"data row50 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row50_col4\" class=\"data row50 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row50_col5\" class=\"data row50 col5\" >bert.encoder.layer.2.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "      <td id=\"T_f4ec0_row51_col0\" class=\"data row51 col0\" >common.blocks.2.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row51_col1\" class=\"data row51 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row51_col2\" class=\"data row51 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row51_col3\" class=\"data row51 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row51_col4\" class=\"data row51 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row51_col5\" class=\"data row51 col5\" >bert.encoder.layer.2.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "      <td id=\"T_f4ec0_row52_col0\" class=\"data row52 col0\" >common.blocks.2.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row52_col1\" class=\"data row52 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row52_col2\" class=\"data row52 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row52_col3\" class=\"data row52 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row52_col4\" class=\"data row52 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row52_col5\" class=\"data row52 col5\" >bert.encoder.layer.2.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "      <td id=\"T_f4ec0_row53_col0\" class=\"data row53 col0\" >common.blocks.2.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row53_col1\" class=\"data row53 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row53_col2\" class=\"data row53 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row53_col3\" class=\"data row53 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row53_col4\" class=\"data row53 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row53_col5\" class=\"data row53 col5\" >bert.encoder.layer.3.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "      <td id=\"T_f4ec0_row54_col0\" class=\"data row54 col0\" >common.blocks.3.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row54_col1\" class=\"data row54 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row54_col2\" class=\"data row54 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row54_col3\" class=\"data row54 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row54_col4\" class=\"data row54 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row54_col5\" class=\"data row54 col5\" >bert.encoder.layer.3.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "      <td id=\"T_f4ec0_row55_col0\" class=\"data row55 col0\" >common.blocks.3.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row55_col1\" class=\"data row55 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row55_col2\" class=\"data row55 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row55_col3\" class=\"data row55 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row55_col4\" class=\"data row55 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row55_col5\" class=\"data row55 col5\" >bert.encoder.layer.3.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "      <td id=\"T_f4ec0_row56_col0\" class=\"data row56 col0\" >common.blocks.3.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row56_col1\" class=\"data row56 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row56_col2\" class=\"data row56 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row56_col3\" class=\"data row56 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row56_col4\" class=\"data row56 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row56_col5\" class=\"data row56 col5\" >bert.encoder.layer.3.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "      <td id=\"T_f4ec0_row57_col0\" class=\"data row57 col0\" >common.blocks.3.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row57_col1\" class=\"data row57 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row57_col2\" class=\"data row57 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row57_col3\" class=\"data row57 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row57_col4\" class=\"data row57 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row57_col5\" class=\"data row57 col5\" >bert.encoder.layer.3.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "      <td id=\"T_f4ec0_row58_col0\" class=\"data row58 col0\" >common.blocks.3.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row58_col1\" class=\"data row58 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row58_col2\" class=\"data row58 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row58_col3\" class=\"data row58 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row58_col4\" class=\"data row58 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row58_col5\" class=\"data row58 col5\" >bert.encoder.layer.3.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "      <td id=\"T_f4ec0_row59_col0\" class=\"data row59 col0\" >common.blocks.3.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row59_col1\" class=\"data row59 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row59_col2\" class=\"data row59 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row59_col3\" class=\"data row59 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row59_col4\" class=\"data row59 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row59_col5\" class=\"data row59 col5\" >bert.encoder.layer.3.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
       "      <td id=\"T_f4ec0_row60_col0\" class=\"data row60 col0\" >common.blocks.3.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row60_col1\" class=\"data row60 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row60_col2\" class=\"data row60 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row60_col3\" class=\"data row60 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row60_col4\" class=\"data row60 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row60_col5\" class=\"data row60 col5\" >bert.encoder.layer.3.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
       "      <td id=\"T_f4ec0_row61_col0\" class=\"data row61 col0\" >common.blocks.3.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row61_col1\" class=\"data row61 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row61_col2\" class=\"data row61 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row61_col3\" class=\"data row61 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row61_col4\" class=\"data row61 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row61_col5\" class=\"data row61 col5\" >bert.encoder.layer.3.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
       "      <td id=\"T_f4ec0_row62_col0\" class=\"data row62 col0\" >common.blocks.3.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row62_col1\" class=\"data row62 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row62_col2\" class=\"data row62 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row62_col3\" class=\"data row62 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row62_col4\" class=\"data row62 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row62_col5\" class=\"data row62 col5\" >bert.encoder.layer.3.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row63\" class=\"row_heading level0 row63\" >63</th>\n",
       "      <td id=\"T_f4ec0_row63_col0\" class=\"data row63 col0\" >common.blocks.3.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row63_col1\" class=\"data row63 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row63_col2\" class=\"data row63 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row63_col3\" class=\"data row63 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row63_col4\" class=\"data row63 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row63_col5\" class=\"data row63 col5\" >bert.encoder.layer.3.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row64\" class=\"row_heading level0 row64\" >64</th>\n",
       "      <td id=\"T_f4ec0_row64_col0\" class=\"data row64 col0\" >common.blocks.3.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row64_col1\" class=\"data row64 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row64_col2\" class=\"data row64 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row64_col3\" class=\"data row64 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row64_col4\" class=\"data row64 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row64_col5\" class=\"data row64 col5\" >bert.encoder.layer.3.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row65\" class=\"row_heading level0 row65\" >65</th>\n",
       "      <td id=\"T_f4ec0_row65_col0\" class=\"data row65 col0\" >common.blocks.3.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row65_col1\" class=\"data row65 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row65_col2\" class=\"data row65 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row65_col3\" class=\"data row65 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row65_col4\" class=\"data row65 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row65_col5\" class=\"data row65 col5\" >bert.encoder.layer.3.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row66\" class=\"row_heading level0 row66\" >66</th>\n",
       "      <td id=\"T_f4ec0_row66_col0\" class=\"data row66 col0\" >common.blocks.3.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row66_col1\" class=\"data row66 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row66_col2\" class=\"data row66 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row66_col3\" class=\"data row66 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row66_col4\" class=\"data row66 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row66_col5\" class=\"data row66 col5\" >bert.encoder.layer.3.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row67\" class=\"row_heading level0 row67\" >67</th>\n",
       "      <td id=\"T_f4ec0_row67_col0\" class=\"data row67 col0\" >common.blocks.3.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row67_col1\" class=\"data row67 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row67_col2\" class=\"data row67 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row67_col3\" class=\"data row67 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row67_col4\" class=\"data row67 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row67_col5\" class=\"data row67 col5\" >bert.encoder.layer.3.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row68\" class=\"row_heading level0 row68\" >68</th>\n",
       "      <td id=\"T_f4ec0_row68_col0\" class=\"data row68 col0\" >common.blocks.3.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row68_col1\" class=\"data row68 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row68_col2\" class=\"data row68 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row68_col3\" class=\"data row68 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row68_col4\" class=\"data row68 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row68_col5\" class=\"data row68 col5\" >bert.encoder.layer.3.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row69\" class=\"row_heading level0 row69\" >69</th>\n",
       "      <td id=\"T_f4ec0_row69_col0\" class=\"data row69 col0\" >common.blocks.3.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row69_col1\" class=\"data row69 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row69_col2\" class=\"data row69 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row69_col3\" class=\"data row69 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row69_col4\" class=\"data row69 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row69_col5\" class=\"data row69 col5\" >bert.encoder.layer.4.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row70\" class=\"row_heading level0 row70\" >70</th>\n",
       "      <td id=\"T_f4ec0_row70_col0\" class=\"data row70 col0\" >common.blocks.4.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row70_col1\" class=\"data row70 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row70_col2\" class=\"data row70 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row70_col3\" class=\"data row70 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row70_col4\" class=\"data row70 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row70_col5\" class=\"data row70 col5\" >bert.encoder.layer.4.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row71\" class=\"row_heading level0 row71\" >71</th>\n",
       "      <td id=\"T_f4ec0_row71_col0\" class=\"data row71 col0\" >common.blocks.4.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row71_col1\" class=\"data row71 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row71_col2\" class=\"data row71 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row71_col3\" class=\"data row71 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row71_col4\" class=\"data row71 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row71_col5\" class=\"data row71 col5\" >bert.encoder.layer.4.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row72\" class=\"row_heading level0 row72\" >72</th>\n",
       "      <td id=\"T_f4ec0_row72_col0\" class=\"data row72 col0\" >common.blocks.4.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row72_col1\" class=\"data row72 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row72_col2\" class=\"data row72 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row72_col3\" class=\"data row72 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row72_col4\" class=\"data row72 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row72_col5\" class=\"data row72 col5\" >bert.encoder.layer.4.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row73\" class=\"row_heading level0 row73\" >73</th>\n",
       "      <td id=\"T_f4ec0_row73_col0\" class=\"data row73 col0\" >common.blocks.4.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row73_col1\" class=\"data row73 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row73_col2\" class=\"data row73 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row73_col3\" class=\"data row73 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row73_col4\" class=\"data row73 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row73_col5\" class=\"data row73 col5\" >bert.encoder.layer.4.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row74\" class=\"row_heading level0 row74\" >74</th>\n",
       "      <td id=\"T_f4ec0_row74_col0\" class=\"data row74 col0\" >common.blocks.4.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row74_col1\" class=\"data row74 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row74_col2\" class=\"data row74 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row74_col3\" class=\"data row74 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row74_col4\" class=\"data row74 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row74_col5\" class=\"data row74 col5\" >bert.encoder.layer.4.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row75\" class=\"row_heading level0 row75\" >75</th>\n",
       "      <td id=\"T_f4ec0_row75_col0\" class=\"data row75 col0\" >common.blocks.4.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row75_col1\" class=\"data row75 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row75_col2\" class=\"data row75 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row75_col3\" class=\"data row75 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row75_col4\" class=\"data row75 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row75_col5\" class=\"data row75 col5\" >bert.encoder.layer.4.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row76\" class=\"row_heading level0 row76\" >76</th>\n",
       "      <td id=\"T_f4ec0_row76_col0\" class=\"data row76 col0\" >common.blocks.4.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row76_col1\" class=\"data row76 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row76_col2\" class=\"data row76 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row76_col3\" class=\"data row76 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row76_col4\" class=\"data row76 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row76_col5\" class=\"data row76 col5\" >bert.encoder.layer.4.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row77\" class=\"row_heading level0 row77\" >77</th>\n",
       "      <td id=\"T_f4ec0_row77_col0\" class=\"data row77 col0\" >common.blocks.4.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row77_col1\" class=\"data row77 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row77_col2\" class=\"data row77 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row77_col3\" class=\"data row77 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row77_col4\" class=\"data row77 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row77_col5\" class=\"data row77 col5\" >bert.encoder.layer.4.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row78\" class=\"row_heading level0 row78\" >78</th>\n",
       "      <td id=\"T_f4ec0_row78_col0\" class=\"data row78 col0\" >common.blocks.4.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row78_col1\" class=\"data row78 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row78_col2\" class=\"data row78 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row78_col3\" class=\"data row78 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row78_col4\" class=\"data row78 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row78_col5\" class=\"data row78 col5\" >bert.encoder.layer.4.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row79\" class=\"row_heading level0 row79\" >79</th>\n",
       "      <td id=\"T_f4ec0_row79_col0\" class=\"data row79 col0\" >common.blocks.4.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row79_col1\" class=\"data row79 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row79_col2\" class=\"data row79 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row79_col3\" class=\"data row79 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row79_col4\" class=\"data row79 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row79_col5\" class=\"data row79 col5\" >bert.encoder.layer.4.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row80\" class=\"row_heading level0 row80\" >80</th>\n",
       "      <td id=\"T_f4ec0_row80_col0\" class=\"data row80 col0\" >common.blocks.4.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row80_col1\" class=\"data row80 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row80_col2\" class=\"data row80 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row80_col3\" class=\"data row80 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row80_col4\" class=\"data row80 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row80_col5\" class=\"data row80 col5\" >bert.encoder.layer.4.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row81\" class=\"row_heading level0 row81\" >81</th>\n",
       "      <td id=\"T_f4ec0_row81_col0\" class=\"data row81 col0\" >common.blocks.4.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row81_col1\" class=\"data row81 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row81_col2\" class=\"data row81 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row81_col3\" class=\"data row81 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row81_col4\" class=\"data row81 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row81_col5\" class=\"data row81 col5\" >bert.encoder.layer.4.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row82\" class=\"row_heading level0 row82\" >82</th>\n",
       "      <td id=\"T_f4ec0_row82_col0\" class=\"data row82 col0\" >common.blocks.4.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row82_col1\" class=\"data row82 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row82_col2\" class=\"data row82 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row82_col3\" class=\"data row82 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row82_col4\" class=\"data row82 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row82_col5\" class=\"data row82 col5\" >bert.encoder.layer.4.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row83\" class=\"row_heading level0 row83\" >83</th>\n",
       "      <td id=\"T_f4ec0_row83_col0\" class=\"data row83 col0\" >common.blocks.4.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row83_col1\" class=\"data row83 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row83_col2\" class=\"data row83 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row83_col3\" class=\"data row83 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row83_col4\" class=\"data row83 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row83_col5\" class=\"data row83 col5\" >bert.encoder.layer.4.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row84\" class=\"row_heading level0 row84\" >84</th>\n",
       "      <td id=\"T_f4ec0_row84_col0\" class=\"data row84 col0\" >common.blocks.4.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row84_col1\" class=\"data row84 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row84_col2\" class=\"data row84 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row84_col3\" class=\"data row84 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row84_col4\" class=\"data row84 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row84_col5\" class=\"data row84 col5\" >bert.encoder.layer.4.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row85\" class=\"row_heading level0 row85\" >85</th>\n",
       "      <td id=\"T_f4ec0_row85_col0\" class=\"data row85 col0\" >common.blocks.4.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row85_col1\" class=\"data row85 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row85_col2\" class=\"data row85 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row85_col3\" class=\"data row85 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row85_col4\" class=\"data row85 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row85_col5\" class=\"data row85 col5\" >bert.encoder.layer.5.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row86\" class=\"row_heading level0 row86\" >86</th>\n",
       "      <td id=\"T_f4ec0_row86_col0\" class=\"data row86 col0\" >common.blocks.5.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row86_col1\" class=\"data row86 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row86_col2\" class=\"data row86 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row86_col3\" class=\"data row86 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row86_col4\" class=\"data row86 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row86_col5\" class=\"data row86 col5\" >bert.encoder.layer.5.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row87\" class=\"row_heading level0 row87\" >87</th>\n",
       "      <td id=\"T_f4ec0_row87_col0\" class=\"data row87 col0\" >common.blocks.5.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row87_col1\" class=\"data row87 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row87_col2\" class=\"data row87 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row87_col3\" class=\"data row87 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row87_col4\" class=\"data row87 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row87_col5\" class=\"data row87 col5\" >bert.encoder.layer.5.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row88\" class=\"row_heading level0 row88\" >88</th>\n",
       "      <td id=\"T_f4ec0_row88_col0\" class=\"data row88 col0\" >common.blocks.5.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row88_col1\" class=\"data row88 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row88_col2\" class=\"data row88 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row88_col3\" class=\"data row88 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row88_col4\" class=\"data row88 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row88_col5\" class=\"data row88 col5\" >bert.encoder.layer.5.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row89\" class=\"row_heading level0 row89\" >89</th>\n",
       "      <td id=\"T_f4ec0_row89_col0\" class=\"data row89 col0\" >common.blocks.5.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row89_col1\" class=\"data row89 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row89_col2\" class=\"data row89 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row89_col3\" class=\"data row89 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row89_col4\" class=\"data row89 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row89_col5\" class=\"data row89 col5\" >bert.encoder.layer.5.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row90\" class=\"row_heading level0 row90\" >90</th>\n",
       "      <td id=\"T_f4ec0_row90_col0\" class=\"data row90 col0\" >common.blocks.5.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row90_col1\" class=\"data row90 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row90_col2\" class=\"data row90 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row90_col3\" class=\"data row90 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row90_col4\" class=\"data row90 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row90_col5\" class=\"data row90 col5\" >bert.encoder.layer.5.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row91\" class=\"row_heading level0 row91\" >91</th>\n",
       "      <td id=\"T_f4ec0_row91_col0\" class=\"data row91 col0\" >common.blocks.5.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row91_col1\" class=\"data row91 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row91_col2\" class=\"data row91 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row91_col3\" class=\"data row91 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row91_col4\" class=\"data row91 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row91_col5\" class=\"data row91 col5\" >bert.encoder.layer.5.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row92\" class=\"row_heading level0 row92\" >92</th>\n",
       "      <td id=\"T_f4ec0_row92_col0\" class=\"data row92 col0\" >common.blocks.5.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row92_col1\" class=\"data row92 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row92_col2\" class=\"data row92 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row92_col3\" class=\"data row92 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row92_col4\" class=\"data row92 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row92_col5\" class=\"data row92 col5\" >bert.encoder.layer.5.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row93\" class=\"row_heading level0 row93\" >93</th>\n",
       "      <td id=\"T_f4ec0_row93_col0\" class=\"data row93 col0\" >common.blocks.5.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row93_col1\" class=\"data row93 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row93_col2\" class=\"data row93 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row93_col3\" class=\"data row93 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row93_col4\" class=\"data row93 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row93_col5\" class=\"data row93 col5\" >bert.encoder.layer.5.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row94\" class=\"row_heading level0 row94\" >94</th>\n",
       "      <td id=\"T_f4ec0_row94_col0\" class=\"data row94 col0\" >common.blocks.5.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row94_col1\" class=\"data row94 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row94_col2\" class=\"data row94 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row94_col3\" class=\"data row94 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row94_col4\" class=\"data row94 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row94_col5\" class=\"data row94 col5\" >bert.encoder.layer.5.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row95\" class=\"row_heading level0 row95\" >95</th>\n",
       "      <td id=\"T_f4ec0_row95_col0\" class=\"data row95 col0\" >common.blocks.5.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row95_col1\" class=\"data row95 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row95_col2\" class=\"data row95 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row95_col3\" class=\"data row95 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row95_col4\" class=\"data row95 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row95_col5\" class=\"data row95 col5\" >bert.encoder.layer.5.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row96\" class=\"row_heading level0 row96\" >96</th>\n",
       "      <td id=\"T_f4ec0_row96_col0\" class=\"data row96 col0\" >common.blocks.5.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row96_col1\" class=\"data row96 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row96_col2\" class=\"data row96 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row96_col3\" class=\"data row96 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row96_col4\" class=\"data row96 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row96_col5\" class=\"data row96 col5\" >bert.encoder.layer.5.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row97\" class=\"row_heading level0 row97\" >97</th>\n",
       "      <td id=\"T_f4ec0_row97_col0\" class=\"data row97 col0\" >common.blocks.5.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row97_col1\" class=\"data row97 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row97_col2\" class=\"data row97 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row97_col3\" class=\"data row97 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row97_col4\" class=\"data row97 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row97_col5\" class=\"data row97 col5\" >bert.encoder.layer.5.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row98\" class=\"row_heading level0 row98\" >98</th>\n",
       "      <td id=\"T_f4ec0_row98_col0\" class=\"data row98 col0\" >common.blocks.5.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row98_col1\" class=\"data row98 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row98_col2\" class=\"data row98 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row98_col3\" class=\"data row98 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row98_col4\" class=\"data row98 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row98_col5\" class=\"data row98 col5\" >bert.encoder.layer.5.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row99\" class=\"row_heading level0 row99\" >99</th>\n",
       "      <td id=\"T_f4ec0_row99_col0\" class=\"data row99 col0\" >common.blocks.5.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row99_col1\" class=\"data row99 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row99_col2\" class=\"data row99 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row99_col3\" class=\"data row99 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row99_col4\" class=\"data row99 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row99_col5\" class=\"data row99 col5\" >bert.encoder.layer.5.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row100\" class=\"row_heading level0 row100\" >100</th>\n",
       "      <td id=\"T_f4ec0_row100_col0\" class=\"data row100 col0\" >common.blocks.5.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row100_col1\" class=\"data row100 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row100_col2\" class=\"data row100 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row100_col3\" class=\"data row100 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row100_col4\" class=\"data row100 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row100_col5\" class=\"data row100 col5\" >bert.encoder.layer.5.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row101\" class=\"row_heading level0 row101\" >101</th>\n",
       "      <td id=\"T_f4ec0_row101_col0\" class=\"data row101 col0\" >common.blocks.5.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row101_col1\" class=\"data row101 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row101_col2\" class=\"data row101 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row101_col3\" class=\"data row101 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row101_col4\" class=\"data row101 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row101_col5\" class=\"data row101 col5\" >bert.encoder.layer.6.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row102\" class=\"row_heading level0 row102\" >102</th>\n",
       "      <td id=\"T_f4ec0_row102_col0\" class=\"data row102 col0\" >common.blocks.6.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row102_col1\" class=\"data row102 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row102_col2\" class=\"data row102 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row102_col3\" class=\"data row102 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row102_col4\" class=\"data row102 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row102_col5\" class=\"data row102 col5\" >bert.encoder.layer.6.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row103\" class=\"row_heading level0 row103\" >103</th>\n",
       "      <td id=\"T_f4ec0_row103_col0\" class=\"data row103 col0\" >common.blocks.6.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row103_col1\" class=\"data row103 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row103_col2\" class=\"data row103 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row103_col3\" class=\"data row103 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row103_col4\" class=\"data row103 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row103_col5\" class=\"data row103 col5\" >bert.encoder.layer.6.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row104\" class=\"row_heading level0 row104\" >104</th>\n",
       "      <td id=\"T_f4ec0_row104_col0\" class=\"data row104 col0\" >common.blocks.6.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row104_col1\" class=\"data row104 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row104_col2\" class=\"data row104 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row104_col3\" class=\"data row104 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row104_col4\" class=\"data row104 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row104_col5\" class=\"data row104 col5\" >bert.encoder.layer.6.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row105\" class=\"row_heading level0 row105\" >105</th>\n",
       "      <td id=\"T_f4ec0_row105_col0\" class=\"data row105 col0\" >common.blocks.6.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row105_col1\" class=\"data row105 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row105_col2\" class=\"data row105 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row105_col3\" class=\"data row105 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row105_col4\" class=\"data row105 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row105_col5\" class=\"data row105 col5\" >bert.encoder.layer.6.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row106\" class=\"row_heading level0 row106\" >106</th>\n",
       "      <td id=\"T_f4ec0_row106_col0\" class=\"data row106 col0\" >common.blocks.6.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row106_col1\" class=\"data row106 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row106_col2\" class=\"data row106 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row106_col3\" class=\"data row106 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row106_col4\" class=\"data row106 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row106_col5\" class=\"data row106 col5\" >bert.encoder.layer.6.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row107\" class=\"row_heading level0 row107\" >107</th>\n",
       "      <td id=\"T_f4ec0_row107_col0\" class=\"data row107 col0\" >common.blocks.6.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row107_col1\" class=\"data row107 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row107_col2\" class=\"data row107 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row107_col3\" class=\"data row107 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row107_col4\" class=\"data row107 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row107_col5\" class=\"data row107 col5\" >bert.encoder.layer.6.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row108\" class=\"row_heading level0 row108\" >108</th>\n",
       "      <td id=\"T_f4ec0_row108_col0\" class=\"data row108 col0\" >common.blocks.6.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row108_col1\" class=\"data row108 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row108_col2\" class=\"data row108 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row108_col3\" class=\"data row108 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row108_col4\" class=\"data row108 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row108_col5\" class=\"data row108 col5\" >bert.encoder.layer.6.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row109\" class=\"row_heading level0 row109\" >109</th>\n",
       "      <td id=\"T_f4ec0_row109_col0\" class=\"data row109 col0\" >common.blocks.6.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row109_col1\" class=\"data row109 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row109_col2\" class=\"data row109 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row109_col3\" class=\"data row109 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row109_col4\" class=\"data row109 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row109_col5\" class=\"data row109 col5\" >bert.encoder.layer.6.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row110\" class=\"row_heading level0 row110\" >110</th>\n",
       "      <td id=\"T_f4ec0_row110_col0\" class=\"data row110 col0\" >common.blocks.6.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row110_col1\" class=\"data row110 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row110_col2\" class=\"data row110 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row110_col3\" class=\"data row110 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row110_col4\" class=\"data row110 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row110_col5\" class=\"data row110 col5\" >bert.encoder.layer.6.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row111\" class=\"row_heading level0 row111\" >111</th>\n",
       "      <td id=\"T_f4ec0_row111_col0\" class=\"data row111 col0\" >common.blocks.6.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row111_col1\" class=\"data row111 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row111_col2\" class=\"data row111 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row111_col3\" class=\"data row111 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row111_col4\" class=\"data row111 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row111_col5\" class=\"data row111 col5\" >bert.encoder.layer.6.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row112\" class=\"row_heading level0 row112\" >112</th>\n",
       "      <td id=\"T_f4ec0_row112_col0\" class=\"data row112 col0\" >common.blocks.6.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row112_col1\" class=\"data row112 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row112_col2\" class=\"data row112 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row112_col3\" class=\"data row112 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row112_col4\" class=\"data row112 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row112_col5\" class=\"data row112 col5\" >bert.encoder.layer.6.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row113\" class=\"row_heading level0 row113\" >113</th>\n",
       "      <td id=\"T_f4ec0_row113_col0\" class=\"data row113 col0\" >common.blocks.6.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row113_col1\" class=\"data row113 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row113_col2\" class=\"data row113 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row113_col3\" class=\"data row113 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row113_col4\" class=\"data row113 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row113_col5\" class=\"data row113 col5\" >bert.encoder.layer.6.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row114\" class=\"row_heading level0 row114\" >114</th>\n",
       "      <td id=\"T_f4ec0_row114_col0\" class=\"data row114 col0\" >common.blocks.6.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row114_col1\" class=\"data row114 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row114_col2\" class=\"data row114 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row114_col3\" class=\"data row114 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row114_col4\" class=\"data row114 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row114_col5\" class=\"data row114 col5\" >bert.encoder.layer.6.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row115\" class=\"row_heading level0 row115\" >115</th>\n",
       "      <td id=\"T_f4ec0_row115_col0\" class=\"data row115 col0\" >common.blocks.6.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row115_col1\" class=\"data row115 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row115_col2\" class=\"data row115 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row115_col3\" class=\"data row115 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row115_col4\" class=\"data row115 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row115_col5\" class=\"data row115 col5\" >bert.encoder.layer.6.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row116\" class=\"row_heading level0 row116\" >116</th>\n",
       "      <td id=\"T_f4ec0_row116_col0\" class=\"data row116 col0\" >common.blocks.6.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row116_col1\" class=\"data row116 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row116_col2\" class=\"data row116 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row116_col3\" class=\"data row116 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row116_col4\" class=\"data row116 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row116_col5\" class=\"data row116 col5\" >bert.encoder.layer.6.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row117\" class=\"row_heading level0 row117\" >117</th>\n",
       "      <td id=\"T_f4ec0_row117_col0\" class=\"data row117 col0\" >common.blocks.6.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row117_col1\" class=\"data row117 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row117_col2\" class=\"data row117 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row117_col3\" class=\"data row117 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row117_col4\" class=\"data row117 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row117_col5\" class=\"data row117 col5\" >bert.encoder.layer.7.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row118\" class=\"row_heading level0 row118\" >118</th>\n",
       "      <td id=\"T_f4ec0_row118_col0\" class=\"data row118 col0\" >common.blocks.7.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row118_col1\" class=\"data row118 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row118_col2\" class=\"data row118 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row118_col3\" class=\"data row118 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row118_col4\" class=\"data row118 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row118_col5\" class=\"data row118 col5\" >bert.encoder.layer.7.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row119\" class=\"row_heading level0 row119\" >119</th>\n",
       "      <td id=\"T_f4ec0_row119_col0\" class=\"data row119 col0\" >common.blocks.7.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row119_col1\" class=\"data row119 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row119_col2\" class=\"data row119 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row119_col3\" class=\"data row119 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row119_col4\" class=\"data row119 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row119_col5\" class=\"data row119 col5\" >bert.encoder.layer.7.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row120\" class=\"row_heading level0 row120\" >120</th>\n",
       "      <td id=\"T_f4ec0_row120_col0\" class=\"data row120 col0\" >common.blocks.7.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row120_col1\" class=\"data row120 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row120_col2\" class=\"data row120 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row120_col3\" class=\"data row120 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row120_col4\" class=\"data row120 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row120_col5\" class=\"data row120 col5\" >bert.encoder.layer.7.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row121\" class=\"row_heading level0 row121\" >121</th>\n",
       "      <td id=\"T_f4ec0_row121_col0\" class=\"data row121 col0\" >common.blocks.7.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row121_col1\" class=\"data row121 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row121_col2\" class=\"data row121 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row121_col3\" class=\"data row121 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row121_col4\" class=\"data row121 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row121_col5\" class=\"data row121 col5\" >bert.encoder.layer.7.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row122\" class=\"row_heading level0 row122\" >122</th>\n",
       "      <td id=\"T_f4ec0_row122_col0\" class=\"data row122 col0\" >common.blocks.7.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row122_col1\" class=\"data row122 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row122_col2\" class=\"data row122 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row122_col3\" class=\"data row122 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row122_col4\" class=\"data row122 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row122_col5\" class=\"data row122 col5\" >bert.encoder.layer.7.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row123\" class=\"row_heading level0 row123\" >123</th>\n",
       "      <td id=\"T_f4ec0_row123_col0\" class=\"data row123 col0\" >common.blocks.7.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row123_col1\" class=\"data row123 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row123_col2\" class=\"data row123 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row123_col3\" class=\"data row123 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row123_col4\" class=\"data row123 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row123_col5\" class=\"data row123 col5\" >bert.encoder.layer.7.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row124\" class=\"row_heading level0 row124\" >124</th>\n",
       "      <td id=\"T_f4ec0_row124_col0\" class=\"data row124 col0\" >common.blocks.7.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row124_col1\" class=\"data row124 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row124_col2\" class=\"data row124 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row124_col3\" class=\"data row124 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row124_col4\" class=\"data row124 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row124_col5\" class=\"data row124 col5\" >bert.encoder.layer.7.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row125\" class=\"row_heading level0 row125\" >125</th>\n",
       "      <td id=\"T_f4ec0_row125_col0\" class=\"data row125 col0\" >common.blocks.7.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row125_col1\" class=\"data row125 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row125_col2\" class=\"data row125 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row125_col3\" class=\"data row125 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row125_col4\" class=\"data row125 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row125_col5\" class=\"data row125 col5\" >bert.encoder.layer.7.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row126\" class=\"row_heading level0 row126\" >126</th>\n",
       "      <td id=\"T_f4ec0_row126_col0\" class=\"data row126 col0\" >common.blocks.7.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row126_col1\" class=\"data row126 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row126_col2\" class=\"data row126 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row126_col3\" class=\"data row126 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row126_col4\" class=\"data row126 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row126_col5\" class=\"data row126 col5\" >bert.encoder.layer.7.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row127\" class=\"row_heading level0 row127\" >127</th>\n",
       "      <td id=\"T_f4ec0_row127_col0\" class=\"data row127 col0\" >common.blocks.7.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row127_col1\" class=\"data row127 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row127_col2\" class=\"data row127 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row127_col3\" class=\"data row127 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row127_col4\" class=\"data row127 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row127_col5\" class=\"data row127 col5\" >bert.encoder.layer.7.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row128\" class=\"row_heading level0 row128\" >128</th>\n",
       "      <td id=\"T_f4ec0_row128_col0\" class=\"data row128 col0\" >common.blocks.7.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row128_col1\" class=\"data row128 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row128_col2\" class=\"data row128 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row128_col3\" class=\"data row128 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row128_col4\" class=\"data row128 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row128_col5\" class=\"data row128 col5\" >bert.encoder.layer.7.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row129\" class=\"row_heading level0 row129\" >129</th>\n",
       "      <td id=\"T_f4ec0_row129_col0\" class=\"data row129 col0\" >common.blocks.7.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row129_col1\" class=\"data row129 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row129_col2\" class=\"data row129 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row129_col3\" class=\"data row129 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row129_col4\" class=\"data row129 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row129_col5\" class=\"data row129 col5\" >bert.encoder.layer.7.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row130\" class=\"row_heading level0 row130\" >130</th>\n",
       "      <td id=\"T_f4ec0_row130_col0\" class=\"data row130 col0\" >common.blocks.7.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row130_col1\" class=\"data row130 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row130_col2\" class=\"data row130 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row130_col3\" class=\"data row130 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row130_col4\" class=\"data row130 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row130_col5\" class=\"data row130 col5\" >bert.encoder.layer.7.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row131\" class=\"row_heading level0 row131\" >131</th>\n",
       "      <td id=\"T_f4ec0_row131_col0\" class=\"data row131 col0\" >common.blocks.7.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row131_col1\" class=\"data row131 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row131_col2\" class=\"data row131 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row131_col3\" class=\"data row131 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row131_col4\" class=\"data row131 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row131_col5\" class=\"data row131 col5\" >bert.encoder.layer.7.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row132\" class=\"row_heading level0 row132\" >132</th>\n",
       "      <td id=\"T_f4ec0_row132_col0\" class=\"data row132 col0\" >common.blocks.7.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row132_col1\" class=\"data row132 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row132_col2\" class=\"data row132 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row132_col3\" class=\"data row132 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row132_col4\" class=\"data row132 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row132_col5\" class=\"data row132 col5\" >bert.encoder.layer.7.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row133\" class=\"row_heading level0 row133\" >133</th>\n",
       "      <td id=\"T_f4ec0_row133_col0\" class=\"data row133 col0\" >common.blocks.7.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row133_col1\" class=\"data row133 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row133_col2\" class=\"data row133 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row133_col3\" class=\"data row133 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row133_col4\" class=\"data row133 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row133_col5\" class=\"data row133 col5\" >bert.encoder.layer.8.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row134\" class=\"row_heading level0 row134\" >134</th>\n",
       "      <td id=\"T_f4ec0_row134_col0\" class=\"data row134 col0\" >common.blocks.8.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row134_col1\" class=\"data row134 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row134_col2\" class=\"data row134 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row134_col3\" class=\"data row134 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row134_col4\" class=\"data row134 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row134_col5\" class=\"data row134 col5\" >bert.encoder.layer.8.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row135\" class=\"row_heading level0 row135\" >135</th>\n",
       "      <td id=\"T_f4ec0_row135_col0\" class=\"data row135 col0\" >common.blocks.8.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row135_col1\" class=\"data row135 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row135_col2\" class=\"data row135 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row135_col3\" class=\"data row135 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row135_col4\" class=\"data row135 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row135_col5\" class=\"data row135 col5\" >bert.encoder.layer.8.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row136\" class=\"row_heading level0 row136\" >136</th>\n",
       "      <td id=\"T_f4ec0_row136_col0\" class=\"data row136 col0\" >common.blocks.8.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row136_col1\" class=\"data row136 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row136_col2\" class=\"data row136 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row136_col3\" class=\"data row136 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row136_col4\" class=\"data row136 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row136_col5\" class=\"data row136 col5\" >bert.encoder.layer.8.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row137\" class=\"row_heading level0 row137\" >137</th>\n",
       "      <td id=\"T_f4ec0_row137_col0\" class=\"data row137 col0\" >common.blocks.8.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row137_col1\" class=\"data row137 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row137_col2\" class=\"data row137 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row137_col3\" class=\"data row137 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row137_col4\" class=\"data row137 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row137_col5\" class=\"data row137 col5\" >bert.encoder.layer.8.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row138\" class=\"row_heading level0 row138\" >138</th>\n",
       "      <td id=\"T_f4ec0_row138_col0\" class=\"data row138 col0\" >common.blocks.8.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row138_col1\" class=\"data row138 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row138_col2\" class=\"data row138 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row138_col3\" class=\"data row138 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row138_col4\" class=\"data row138 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row138_col5\" class=\"data row138 col5\" >bert.encoder.layer.8.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row139\" class=\"row_heading level0 row139\" >139</th>\n",
       "      <td id=\"T_f4ec0_row139_col0\" class=\"data row139 col0\" >common.blocks.8.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row139_col1\" class=\"data row139 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row139_col2\" class=\"data row139 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row139_col3\" class=\"data row139 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row139_col4\" class=\"data row139 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row139_col5\" class=\"data row139 col5\" >bert.encoder.layer.8.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row140\" class=\"row_heading level0 row140\" >140</th>\n",
       "      <td id=\"T_f4ec0_row140_col0\" class=\"data row140 col0\" >common.blocks.8.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row140_col1\" class=\"data row140 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row140_col2\" class=\"data row140 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row140_col3\" class=\"data row140 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row140_col4\" class=\"data row140 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row140_col5\" class=\"data row140 col5\" >bert.encoder.layer.8.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row141\" class=\"row_heading level0 row141\" >141</th>\n",
       "      <td id=\"T_f4ec0_row141_col0\" class=\"data row141 col0\" >common.blocks.8.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row141_col1\" class=\"data row141 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row141_col2\" class=\"data row141 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row141_col3\" class=\"data row141 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row141_col4\" class=\"data row141 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row141_col5\" class=\"data row141 col5\" >bert.encoder.layer.8.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row142\" class=\"row_heading level0 row142\" >142</th>\n",
       "      <td id=\"T_f4ec0_row142_col0\" class=\"data row142 col0\" >common.blocks.8.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row142_col1\" class=\"data row142 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row142_col2\" class=\"data row142 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row142_col3\" class=\"data row142 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row142_col4\" class=\"data row142 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row142_col5\" class=\"data row142 col5\" >bert.encoder.layer.8.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row143\" class=\"row_heading level0 row143\" >143</th>\n",
       "      <td id=\"T_f4ec0_row143_col0\" class=\"data row143 col0\" >common.blocks.8.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row143_col1\" class=\"data row143 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row143_col2\" class=\"data row143 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row143_col3\" class=\"data row143 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row143_col4\" class=\"data row143 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row143_col5\" class=\"data row143 col5\" >bert.encoder.layer.8.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row144\" class=\"row_heading level0 row144\" >144</th>\n",
       "      <td id=\"T_f4ec0_row144_col0\" class=\"data row144 col0\" >common.blocks.8.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row144_col1\" class=\"data row144 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row144_col2\" class=\"data row144 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row144_col3\" class=\"data row144 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row144_col4\" class=\"data row144 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row144_col5\" class=\"data row144 col5\" >bert.encoder.layer.8.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row145\" class=\"row_heading level0 row145\" >145</th>\n",
       "      <td id=\"T_f4ec0_row145_col0\" class=\"data row145 col0\" >common.blocks.8.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row145_col1\" class=\"data row145 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row145_col2\" class=\"data row145 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row145_col3\" class=\"data row145 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row145_col4\" class=\"data row145 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row145_col5\" class=\"data row145 col5\" >bert.encoder.layer.8.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row146\" class=\"row_heading level0 row146\" >146</th>\n",
       "      <td id=\"T_f4ec0_row146_col0\" class=\"data row146 col0\" >common.blocks.8.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row146_col1\" class=\"data row146 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row146_col2\" class=\"data row146 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row146_col3\" class=\"data row146 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row146_col4\" class=\"data row146 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row146_col5\" class=\"data row146 col5\" >bert.encoder.layer.8.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row147\" class=\"row_heading level0 row147\" >147</th>\n",
       "      <td id=\"T_f4ec0_row147_col0\" class=\"data row147 col0\" >common.blocks.8.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row147_col1\" class=\"data row147 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row147_col2\" class=\"data row147 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row147_col3\" class=\"data row147 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row147_col4\" class=\"data row147 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row147_col5\" class=\"data row147 col5\" >bert.encoder.layer.8.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row148\" class=\"row_heading level0 row148\" >148</th>\n",
       "      <td id=\"T_f4ec0_row148_col0\" class=\"data row148 col0\" >common.blocks.8.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row148_col1\" class=\"data row148 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row148_col2\" class=\"data row148 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row148_col3\" class=\"data row148 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row148_col4\" class=\"data row148 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row148_col5\" class=\"data row148 col5\" >bert.encoder.layer.8.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row149\" class=\"row_heading level0 row149\" >149</th>\n",
       "      <td id=\"T_f4ec0_row149_col0\" class=\"data row149 col0\" >common.blocks.8.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row149_col1\" class=\"data row149 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row149_col2\" class=\"data row149 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row149_col3\" class=\"data row149 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row149_col4\" class=\"data row149 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row149_col5\" class=\"data row149 col5\" >bert.encoder.layer.9.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row150\" class=\"row_heading level0 row150\" >150</th>\n",
       "      <td id=\"T_f4ec0_row150_col0\" class=\"data row150 col0\" >common.blocks.9.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row150_col1\" class=\"data row150 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row150_col2\" class=\"data row150 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row150_col3\" class=\"data row150 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row150_col4\" class=\"data row150 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row150_col5\" class=\"data row150 col5\" >bert.encoder.layer.9.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row151\" class=\"row_heading level0 row151\" >151</th>\n",
       "      <td id=\"T_f4ec0_row151_col0\" class=\"data row151 col0\" >common.blocks.9.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row151_col1\" class=\"data row151 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row151_col2\" class=\"data row151 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row151_col3\" class=\"data row151 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row151_col4\" class=\"data row151 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row151_col5\" class=\"data row151 col5\" >bert.encoder.layer.9.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row152\" class=\"row_heading level0 row152\" >152</th>\n",
       "      <td id=\"T_f4ec0_row152_col0\" class=\"data row152 col0\" >common.blocks.9.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row152_col1\" class=\"data row152 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row152_col2\" class=\"data row152 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row152_col3\" class=\"data row152 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row152_col4\" class=\"data row152 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row152_col5\" class=\"data row152 col5\" >bert.encoder.layer.9.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row153\" class=\"row_heading level0 row153\" >153</th>\n",
       "      <td id=\"T_f4ec0_row153_col0\" class=\"data row153 col0\" >common.blocks.9.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row153_col1\" class=\"data row153 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row153_col2\" class=\"data row153 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row153_col3\" class=\"data row153 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row153_col4\" class=\"data row153 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row153_col5\" class=\"data row153 col5\" >bert.encoder.layer.9.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row154\" class=\"row_heading level0 row154\" >154</th>\n",
       "      <td id=\"T_f4ec0_row154_col0\" class=\"data row154 col0\" >common.blocks.9.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row154_col1\" class=\"data row154 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row154_col2\" class=\"data row154 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row154_col3\" class=\"data row154 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row154_col4\" class=\"data row154 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row154_col5\" class=\"data row154 col5\" >bert.encoder.layer.9.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row155\" class=\"row_heading level0 row155\" >155</th>\n",
       "      <td id=\"T_f4ec0_row155_col0\" class=\"data row155 col0\" >common.blocks.9.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row155_col1\" class=\"data row155 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row155_col2\" class=\"data row155 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row155_col3\" class=\"data row155 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row155_col4\" class=\"data row155 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row155_col5\" class=\"data row155 col5\" >bert.encoder.layer.9.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row156\" class=\"row_heading level0 row156\" >156</th>\n",
       "      <td id=\"T_f4ec0_row156_col0\" class=\"data row156 col0\" >common.blocks.9.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row156_col1\" class=\"data row156 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row156_col2\" class=\"data row156 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row156_col3\" class=\"data row156 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row156_col4\" class=\"data row156 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row156_col5\" class=\"data row156 col5\" >bert.encoder.layer.9.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row157\" class=\"row_heading level0 row157\" >157</th>\n",
       "      <td id=\"T_f4ec0_row157_col0\" class=\"data row157 col0\" >common.blocks.9.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row157_col1\" class=\"data row157 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row157_col2\" class=\"data row157 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row157_col3\" class=\"data row157 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row157_col4\" class=\"data row157 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row157_col5\" class=\"data row157 col5\" >bert.encoder.layer.9.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row158\" class=\"row_heading level0 row158\" >158</th>\n",
       "      <td id=\"T_f4ec0_row158_col0\" class=\"data row158 col0\" >common.blocks.9.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row158_col1\" class=\"data row158 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row158_col2\" class=\"data row158 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row158_col3\" class=\"data row158 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row158_col4\" class=\"data row158 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row158_col5\" class=\"data row158 col5\" >bert.encoder.layer.9.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row159\" class=\"row_heading level0 row159\" >159</th>\n",
       "      <td id=\"T_f4ec0_row159_col0\" class=\"data row159 col0\" >common.blocks.9.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row159_col1\" class=\"data row159 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row159_col2\" class=\"data row159 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row159_col3\" class=\"data row159 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row159_col4\" class=\"data row159 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row159_col5\" class=\"data row159 col5\" >bert.encoder.layer.9.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row160\" class=\"row_heading level0 row160\" >160</th>\n",
       "      <td id=\"T_f4ec0_row160_col0\" class=\"data row160 col0\" >common.blocks.9.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row160_col1\" class=\"data row160 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row160_col2\" class=\"data row160 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row160_col3\" class=\"data row160 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row160_col4\" class=\"data row160 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row160_col5\" class=\"data row160 col5\" >bert.encoder.layer.9.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row161\" class=\"row_heading level0 row161\" >161</th>\n",
       "      <td id=\"T_f4ec0_row161_col0\" class=\"data row161 col0\" >common.blocks.9.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row161_col1\" class=\"data row161 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row161_col2\" class=\"data row161 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row161_col3\" class=\"data row161 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row161_col4\" class=\"data row161 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row161_col5\" class=\"data row161 col5\" >bert.encoder.layer.9.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row162\" class=\"row_heading level0 row162\" >162</th>\n",
       "      <td id=\"T_f4ec0_row162_col0\" class=\"data row162 col0\" >common.blocks.9.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row162_col1\" class=\"data row162 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row162_col2\" class=\"data row162 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row162_col3\" class=\"data row162 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row162_col4\" class=\"data row162 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row162_col5\" class=\"data row162 col5\" >bert.encoder.layer.9.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row163\" class=\"row_heading level0 row163\" >163</th>\n",
       "      <td id=\"T_f4ec0_row163_col0\" class=\"data row163 col0\" >common.blocks.9.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row163_col1\" class=\"data row163 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row163_col2\" class=\"data row163 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row163_col3\" class=\"data row163 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row163_col4\" class=\"data row163 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row163_col5\" class=\"data row163 col5\" >bert.encoder.layer.9.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row164\" class=\"row_heading level0 row164\" >164</th>\n",
       "      <td id=\"T_f4ec0_row164_col0\" class=\"data row164 col0\" >common.blocks.9.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row164_col1\" class=\"data row164 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row164_col2\" class=\"data row164 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row164_col3\" class=\"data row164 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row164_col4\" class=\"data row164 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row164_col5\" class=\"data row164 col5\" >bert.encoder.layer.9.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row165\" class=\"row_heading level0 row165\" >165</th>\n",
       "      <td id=\"T_f4ec0_row165_col0\" class=\"data row165 col0\" >common.blocks.9.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row165_col1\" class=\"data row165 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row165_col2\" class=\"data row165 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row165_col3\" class=\"data row165 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row165_col4\" class=\"data row165 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row165_col5\" class=\"data row165 col5\" >bert.encoder.layer.10.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row166\" class=\"row_heading level0 row166\" >166</th>\n",
       "      <td id=\"T_f4ec0_row166_col0\" class=\"data row166 col0\" >common.blocks.10.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row166_col1\" class=\"data row166 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row166_col2\" class=\"data row166 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row166_col3\" class=\"data row166 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row166_col4\" class=\"data row166 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row166_col5\" class=\"data row166 col5\" >bert.encoder.layer.10.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row167\" class=\"row_heading level0 row167\" >167</th>\n",
       "      <td id=\"T_f4ec0_row167_col0\" class=\"data row167 col0\" >common.blocks.10.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row167_col1\" class=\"data row167 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row167_col2\" class=\"data row167 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row167_col3\" class=\"data row167 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row167_col4\" class=\"data row167 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row167_col5\" class=\"data row167 col5\" >bert.encoder.layer.10.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row168\" class=\"row_heading level0 row168\" >168</th>\n",
       "      <td id=\"T_f4ec0_row168_col0\" class=\"data row168 col0\" >common.blocks.10.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row168_col1\" class=\"data row168 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row168_col2\" class=\"data row168 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row168_col3\" class=\"data row168 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row168_col4\" class=\"data row168 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row168_col5\" class=\"data row168 col5\" >bert.encoder.layer.10.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row169\" class=\"row_heading level0 row169\" >169</th>\n",
       "      <td id=\"T_f4ec0_row169_col0\" class=\"data row169 col0\" >common.blocks.10.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row169_col1\" class=\"data row169 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row169_col2\" class=\"data row169 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row169_col3\" class=\"data row169 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row169_col4\" class=\"data row169 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row169_col5\" class=\"data row169 col5\" >bert.encoder.layer.10.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row170\" class=\"row_heading level0 row170\" >170</th>\n",
       "      <td id=\"T_f4ec0_row170_col0\" class=\"data row170 col0\" >common.blocks.10.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row170_col1\" class=\"data row170 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row170_col2\" class=\"data row170 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row170_col3\" class=\"data row170 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row170_col4\" class=\"data row170 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row170_col5\" class=\"data row170 col5\" >bert.encoder.layer.10.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row171\" class=\"row_heading level0 row171\" >171</th>\n",
       "      <td id=\"T_f4ec0_row171_col0\" class=\"data row171 col0\" >common.blocks.10.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row171_col1\" class=\"data row171 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row171_col2\" class=\"data row171 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row171_col3\" class=\"data row171 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row171_col4\" class=\"data row171 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row171_col5\" class=\"data row171 col5\" >bert.encoder.layer.10.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row172\" class=\"row_heading level0 row172\" >172</th>\n",
       "      <td id=\"T_f4ec0_row172_col0\" class=\"data row172 col0\" >common.blocks.10.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row172_col1\" class=\"data row172 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row172_col2\" class=\"data row172 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row172_col3\" class=\"data row172 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row172_col4\" class=\"data row172 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row172_col5\" class=\"data row172 col5\" >bert.encoder.layer.10.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row173\" class=\"row_heading level0 row173\" >173</th>\n",
       "      <td id=\"T_f4ec0_row173_col0\" class=\"data row173 col0\" >common.blocks.10.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row173_col1\" class=\"data row173 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row173_col2\" class=\"data row173 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row173_col3\" class=\"data row173 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row173_col4\" class=\"data row173 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row173_col5\" class=\"data row173 col5\" >bert.encoder.layer.10.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row174\" class=\"row_heading level0 row174\" >174</th>\n",
       "      <td id=\"T_f4ec0_row174_col0\" class=\"data row174 col0\" >common.blocks.10.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row174_col1\" class=\"data row174 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row174_col2\" class=\"data row174 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row174_col3\" class=\"data row174 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row174_col4\" class=\"data row174 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row174_col5\" class=\"data row174 col5\" >bert.encoder.layer.10.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row175\" class=\"row_heading level0 row175\" >175</th>\n",
       "      <td id=\"T_f4ec0_row175_col0\" class=\"data row175 col0\" >common.blocks.10.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row175_col1\" class=\"data row175 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row175_col2\" class=\"data row175 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row175_col3\" class=\"data row175 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row175_col4\" class=\"data row175 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row175_col5\" class=\"data row175 col5\" >bert.encoder.layer.10.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row176\" class=\"row_heading level0 row176\" >176</th>\n",
       "      <td id=\"T_f4ec0_row176_col0\" class=\"data row176 col0\" >common.blocks.10.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row176_col1\" class=\"data row176 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row176_col2\" class=\"data row176 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row176_col3\" class=\"data row176 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row176_col4\" class=\"data row176 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row176_col5\" class=\"data row176 col5\" >bert.encoder.layer.10.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row177\" class=\"row_heading level0 row177\" >177</th>\n",
       "      <td id=\"T_f4ec0_row177_col0\" class=\"data row177 col0\" >common.blocks.10.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row177_col1\" class=\"data row177 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row177_col2\" class=\"data row177 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row177_col3\" class=\"data row177 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row177_col4\" class=\"data row177 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row177_col5\" class=\"data row177 col5\" >bert.encoder.layer.10.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row178\" class=\"row_heading level0 row178\" >178</th>\n",
       "      <td id=\"T_f4ec0_row178_col0\" class=\"data row178 col0\" >common.blocks.10.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row178_col1\" class=\"data row178 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row178_col2\" class=\"data row178 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row178_col3\" class=\"data row178 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row178_col4\" class=\"data row178 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row178_col5\" class=\"data row178 col5\" >bert.encoder.layer.10.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row179\" class=\"row_heading level0 row179\" >179</th>\n",
       "      <td id=\"T_f4ec0_row179_col0\" class=\"data row179 col0\" >common.blocks.10.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row179_col1\" class=\"data row179 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row179_col2\" class=\"data row179 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row179_col3\" class=\"data row179 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row179_col4\" class=\"data row179 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row179_col5\" class=\"data row179 col5\" >bert.encoder.layer.10.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row180\" class=\"row_heading level0 row180\" >180</th>\n",
       "      <td id=\"T_f4ec0_row180_col0\" class=\"data row180 col0\" >common.blocks.10.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row180_col1\" class=\"data row180 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row180_col2\" class=\"data row180 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row180_col3\" class=\"data row180 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row180_col4\" class=\"data row180 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row180_col5\" class=\"data row180 col5\" >bert.encoder.layer.10.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row181\" class=\"row_heading level0 row181\" >181</th>\n",
       "      <td id=\"T_f4ec0_row181_col0\" class=\"data row181 col0\" >common.blocks.10.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row181_col1\" class=\"data row181 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row181_col2\" class=\"data row181 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row181_col3\" class=\"data row181 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row181_col4\" class=\"data row181 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row181_col5\" class=\"data row181 col5\" >bert.encoder.layer.11.attention.self.query.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row182\" class=\"row_heading level0 row182\" >182</th>\n",
       "      <td id=\"T_f4ec0_row182_col0\" class=\"data row182 col0\" >common.blocks.11.attn.W_Q.weight</td>\n",
       "      <td id=\"T_f4ec0_row182_col1\" class=\"data row182 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row182_col2\" class=\"data row182 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row182_col3\" class=\"data row182 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row182_col4\" class=\"data row182 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row182_col5\" class=\"data row182 col5\" >bert.encoder.layer.11.attention.self.query.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row183\" class=\"row_heading level0 row183\" >183</th>\n",
       "      <td id=\"T_f4ec0_row183_col0\" class=\"data row183 col0\" >common.blocks.11.attn.W_Q.bias</td>\n",
       "      <td id=\"T_f4ec0_row183_col1\" class=\"data row183 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row183_col2\" class=\"data row183 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row183_col3\" class=\"data row183 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row183_col4\" class=\"data row183 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row183_col5\" class=\"data row183 col5\" >bert.encoder.layer.11.attention.self.key.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row184\" class=\"row_heading level0 row184\" >184</th>\n",
       "      <td id=\"T_f4ec0_row184_col0\" class=\"data row184 col0\" >common.blocks.11.attn.W_K.weight</td>\n",
       "      <td id=\"T_f4ec0_row184_col1\" class=\"data row184 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row184_col2\" class=\"data row184 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row184_col3\" class=\"data row184 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row184_col4\" class=\"data row184 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row184_col5\" class=\"data row184 col5\" >bert.encoder.layer.11.attention.self.key.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row185\" class=\"row_heading level0 row185\" >185</th>\n",
       "      <td id=\"T_f4ec0_row185_col0\" class=\"data row185 col0\" >common.blocks.11.attn.W_K.bias</td>\n",
       "      <td id=\"T_f4ec0_row185_col1\" class=\"data row185 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row185_col2\" class=\"data row185 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row185_col3\" class=\"data row185 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row185_col4\" class=\"data row185 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row185_col5\" class=\"data row185 col5\" >bert.encoder.layer.11.attention.self.value.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row186\" class=\"row_heading level0 row186\" >186</th>\n",
       "      <td id=\"T_f4ec0_row186_col0\" class=\"data row186 col0\" >common.blocks.11.attn.W_V.weight</td>\n",
       "      <td id=\"T_f4ec0_row186_col1\" class=\"data row186 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row186_col2\" class=\"data row186 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row186_col3\" class=\"data row186 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row186_col4\" class=\"data row186 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row186_col5\" class=\"data row186 col5\" >bert.encoder.layer.11.attention.self.value.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row187\" class=\"row_heading level0 row187\" >187</th>\n",
       "      <td id=\"T_f4ec0_row187_col0\" class=\"data row187 col0\" >common.blocks.11.attn.W_V.bias</td>\n",
       "      <td id=\"T_f4ec0_row187_col1\" class=\"data row187 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row187_col2\" class=\"data row187 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row187_col3\" class=\"data row187 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row187_col4\" class=\"data row187 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row187_col5\" class=\"data row187 col5\" >bert.encoder.layer.11.attention.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row188\" class=\"row_heading level0 row188\" >188</th>\n",
       "      <td id=\"T_f4ec0_row188_col0\" class=\"data row188 col0\" >common.blocks.11.attn.ff.weight</td>\n",
       "      <td id=\"T_f4ec0_row188_col1\" class=\"data row188 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row188_col2\" class=\"data row188 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row188_col3\" class=\"data row188 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row188_col4\" class=\"data row188 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row188_col5\" class=\"data row188 col5\" >bert.encoder.layer.11.attention.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row189\" class=\"row_heading level0 row189\" >189</th>\n",
       "      <td id=\"T_f4ec0_row189_col0\" class=\"data row189 col0\" >common.blocks.11.attn.ff.bias</td>\n",
       "      <td id=\"T_f4ec0_row189_col1\" class=\"data row189 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row189_col2\" class=\"data row189 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row189_col3\" class=\"data row189 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row189_col4\" class=\"data row189 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row189_col5\" class=\"data row189 col5\" >bert.encoder.layer.11.attention.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row190\" class=\"row_heading level0 row190\" >190</th>\n",
       "      <td id=\"T_f4ec0_row190_col0\" class=\"data row190 col0\" >common.blocks.11.lnorm1.weight</td>\n",
       "      <td id=\"T_f4ec0_row190_col1\" class=\"data row190 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row190_col2\" class=\"data row190 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row190_col3\" class=\"data row190 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row190_col4\" class=\"data row190 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row190_col5\" class=\"data row190 col5\" >bert.encoder.layer.11.attention.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row191\" class=\"row_heading level0 row191\" >191</th>\n",
       "      <td id=\"T_f4ec0_row191_col0\" class=\"data row191 col0\" >common.blocks.11.lnorm1.bias</td>\n",
       "      <td id=\"T_f4ec0_row191_col1\" class=\"data row191 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row191_col2\" class=\"data row191 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row191_col3\" class=\"data row191 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row191_col4\" class=\"data row191 col4\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row191_col5\" class=\"data row191 col5\" >bert.encoder.layer.11.intermediate.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row192\" class=\"row_heading level0 row192\" >192</th>\n",
       "      <td id=\"T_f4ec0_row192_col0\" class=\"data row192 col0\" >common.blocks.11.mlp.linear1.weight</td>\n",
       "      <td id=\"T_f4ec0_row192_col1\" class=\"data row192 col1\" >(3072, 768)</td>\n",
       "      <td id=\"T_f4ec0_row192_col2\" class=\"data row192 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row192_col3\" class=\"data row192 col3\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row192_col4\" class=\"data row192 col4\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row192_col5\" class=\"data row192 col5\" >bert.encoder.layer.11.intermediate.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row193\" class=\"row_heading level0 row193\" >193</th>\n",
       "      <td id=\"T_f4ec0_row193_col0\" class=\"data row193 col0\" >common.blocks.11.mlp.linear1.bias</td>\n",
       "      <td id=\"T_f4ec0_row193_col1\" class=\"data row193 col1\" >(3072,)</td>\n",
       "      <td id=\"T_f4ec0_row193_col2\" class=\"data row193 col2\" >3072</td>\n",
       "      <td id=\"T_f4ec0_row193_col3\" class=\"data row193 col3\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row193_col4\" class=\"data row193 col4\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row193_col5\" class=\"data row193 col5\" >bert.encoder.layer.11.output.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row194\" class=\"row_heading level0 row194\" >194</th>\n",
       "      <td id=\"T_f4ec0_row194_col0\" class=\"data row194 col0\" >common.blocks.11.mlp.linear2.weight</td>\n",
       "      <td id=\"T_f4ec0_row194_col1\" class=\"data row194 col1\" >(768, 3072)</td>\n",
       "      <td id=\"T_f4ec0_row194_col2\" class=\"data row194 col2\" >2359296</td>\n",
       "      <td id=\"T_f4ec0_row194_col3\" class=\"data row194 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row194_col4\" class=\"data row194 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row194_col5\" class=\"data row194 col5\" >bert.encoder.layer.11.output.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row195\" class=\"row_heading level0 row195\" >195</th>\n",
       "      <td id=\"T_f4ec0_row195_col0\" class=\"data row195 col0\" >common.blocks.11.mlp.linear2.bias</td>\n",
       "      <td id=\"T_f4ec0_row195_col1\" class=\"data row195 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row195_col2\" class=\"data row195 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row195_col3\" class=\"data row195 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row195_col4\" class=\"data row195 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row195_col5\" class=\"data row195 col5\" >bert.encoder.layer.11.output.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row196\" class=\"row_heading level0 row196\" >196</th>\n",
       "      <td id=\"T_f4ec0_row196_col0\" class=\"data row196 col0\" >common.blocks.11.lnorm2.weight</td>\n",
       "      <td id=\"T_f4ec0_row196_col1\" class=\"data row196 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row196_col2\" class=\"data row196 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row196_col3\" class=\"data row196 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row196_col4\" class=\"data row196 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row196_col5\" class=\"data row196 col5\" >bert.encoder.layer.11.output.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row197\" class=\"row_heading level0 row197\" >197</th>\n",
       "      <td id=\"T_f4ec0_row197_col0\" class=\"data row197 col0\" >common.blocks.11.lnorm2.bias</td>\n",
       "      <td id=\"T_f4ec0_row197_col1\" class=\"data row197 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row197_col2\" class=\"data row197 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row197_col3\" class=\"data row197 col3\" >28996</td>\n",
       "      <td id=\"T_f4ec0_row197_col4\" class=\"data row197 col4\" >(28996,)</td>\n",
       "      <td id=\"T_f4ec0_row197_col5\" class=\"data row197 col5\" >cls.predictions.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row198\" class=\"row_heading level0 row198\" >198</th>\n",
       "      <td id=\"T_f4ec0_row198_col0\" class=\"data row198 col0\" >linear.weight</td>\n",
       "      <td id=\"T_f4ec0_row198_col1\" class=\"data row198 col1\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row198_col2\" class=\"data row198 col2\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row198_col3\" class=\"data row198 col3\" >589824</td>\n",
       "      <td id=\"T_f4ec0_row198_col4\" class=\"data row198 col4\" >(768, 768)</td>\n",
       "      <td id=\"T_f4ec0_row198_col5\" class=\"data row198 col5\" >cls.predictions.transform.dense.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row199\" class=\"row_heading level0 row199\" >199</th>\n",
       "      <td id=\"T_f4ec0_row199_col0\" class=\"data row199 col0\" >linear.bias</td>\n",
       "      <td id=\"T_f4ec0_row199_col1\" class=\"data row199 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row199_col2\" class=\"data row199 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row199_col3\" class=\"data row199 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row199_col4\" class=\"data row199 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row199_col5\" class=\"data row199 col5\" >cls.predictions.transform.dense.bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row200\" class=\"row_heading level0 row200\" >200</th>\n",
       "      <td id=\"T_f4ec0_row200_col0\" class=\"data row200 col0\" >lnorm.weight</td>\n",
       "      <td id=\"T_f4ec0_row200_col1\" class=\"data row200 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row200_col2\" class=\"data row200 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row200_col3\" class=\"data row200 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row200_col4\" class=\"data row200 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row200_col5\" class=\"data row200 col5\" >cls.predictions.transform.LayerNorm.weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4ec0_level0_row201\" class=\"row_heading level0 row201\" >201</th>\n",
       "      <td id=\"T_f4ec0_row201_col0\" class=\"data row201 col0\" >lnorm.bias</td>\n",
       "      <td id=\"T_f4ec0_row201_col1\" class=\"data row201 col1\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row201_col2\" class=\"data row201 col2\" >768</td>\n",
       "      <td id=\"T_f4ec0_row201_col3\" class=\"data row201 col3\" >768</td>\n",
       "      <td id=\"T_f4ec0_row201_col4\" class=\"data row201 col4\" >(768,)</td>\n",
       "      <td id=\"T_f4ec0_row201_col5\" class=\"data row201 col5\" >cls.predictions.transform.LayerNorm.bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f371c857400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: The offset between weights in my version of BERT and the original is expected.\n",
    "# The original BERT uses a different embedding layer. The weights are the same, but the bias is different.\n",
    "# The function below for copying weights compensates for the difference.\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "my_bert = BERTLanguageModel(config).train()\n",
    "bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "utils.print_param_count(my_bert, bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name: common.emb.weight my size: torch.Size([28996, 768])\n",
      "bert name: bert.embeddings.word_embeddings.weight bert size: torch.Size([28996, 768])\n",
      "my name: common.pos_emb.weight my size: torch.Size([512, 768])\n",
      "bert name: bert.embeddings.position_embeddings.weight bert size: torch.Size([512, 768])\n",
      "my name: common.tkn_emb.weight my size: torch.Size([2, 768])\n",
      "bert name: bert.embeddings.token_type_embeddings.weight bert size: torch.Size([2, 768])\n",
      "my name: common.lnorm.weight my size: torch.Size([768])\n",
      "bert name: bert.embeddings.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.lnorm.bias my size: torch.Size([768])\n",
      "bert name: bert.embeddings.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.0.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.0.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.0.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.0.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.0.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.0.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.0.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.0.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.0.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.0.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.0.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.0.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.0.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.0.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.0.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.0.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.0.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.0.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.1.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.1.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.1.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.1.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.1.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.1.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.1.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.1.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.1.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.1.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.1.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.1.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.1.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.1.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.1.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.1.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.1.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.1.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.2.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.2.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.2.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.2.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.2.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.2.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.2.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.2.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.2.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.2.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.2.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.2.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.2.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.2.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.2.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.2.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.2.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.2.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.3.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.3.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.3.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.3.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.3.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.3.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.3.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.3.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.3.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.3.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.3.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.3.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.3.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.3.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.3.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.3.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.3.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.3.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.4.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.4.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.4.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.4.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.4.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.4.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.4.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.4.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.4.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.4.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.4.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.4.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.4.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.4.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.4.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.4.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.4.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.4.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.5.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.5.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.5.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.5.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.5.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.5.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.5.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.5.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.5.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.5.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.5.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.5.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.5.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.5.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.5.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.5.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.5.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.5.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.6.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.6.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.6.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.6.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.6.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.6.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.6.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.6.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.6.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.6.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.6.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.6.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.6.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.6.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.6.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.6.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.6.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.6.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.7.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.7.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.7.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.7.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.7.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.7.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.7.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.7.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.7.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.7.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.7.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.7.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.7.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.7.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.7.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.7.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.7.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.7.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.8.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.8.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.8.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.8.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.8.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.8.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.8.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.8.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.8.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.8.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.8.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.8.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.8.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.8.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.8.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.8.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.8.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.8.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.9.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.9.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.9.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.9.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.9.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.9.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.9.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.9.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.9.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.9.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.9.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.9.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.9.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.9.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.9.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.9.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.9.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.9.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.10.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.10.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.10.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.10.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.10.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.10.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.10.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.10.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.10.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.10.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.10.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.10.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.10.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.10.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.10.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.10.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.10.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.10.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.attn.W_Q.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.11.attention.self.query.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.11.attn.W_Q.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.self.query.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.attn.W_K.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.11.attention.self.key.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.11.attn.W_K.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.self.key.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.attn.W_V.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.11.attention.self.value.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.11.attn.W_V.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.self.value.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.attn.ff.weight my size: torch.Size([768, 768])\n",
      "bert name: bert.encoder.layer.11.attention.output.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: common.blocks.11.attn.ff.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.lnorm1.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.11.lnorm1.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.attention.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.mlp.linear1.weight my size: torch.Size([3072, 768])\n",
      "bert name: bert.encoder.layer.11.intermediate.dense.weight bert size: torch.Size([3072, 768])\n",
      "my name: common.blocks.11.mlp.linear1.bias my size: torch.Size([3072])\n",
      "bert name: bert.encoder.layer.11.intermediate.dense.bias bert size: torch.Size([3072])\n",
      "my name: common.blocks.11.mlp.linear2.weight my size: torch.Size([768, 3072])\n",
      "bert name: bert.encoder.layer.11.output.dense.weight bert size: torch.Size([768, 3072])\n",
      "my name: common.blocks.11.mlp.linear2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.output.dense.bias bert size: torch.Size([768])\n",
      "my name: common.blocks.11.lnorm2.weight my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.output.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: common.blocks.11.lnorm2.bias my size: torch.Size([768])\n",
      "bert name: bert.encoder.layer.11.output.LayerNorm.bias bert size: torch.Size([768])\n",
      "my name: tied_embed_bias my size: torch.Size([28996])\n",
      "bert name: cls.predictions.bias bert size: torch.Size([28996])\n",
      "my name: linear.weight my size: torch.Size([768, 768])\n",
      "bert name: cls.predictions.transform.dense.weight bert size: torch.Size([768, 768])\n",
      "my name: linear.bias my size: torch.Size([768])\n",
      "bert name: cls.predictions.transform.dense.bias bert size: torch.Size([768])\n",
      "my name: lnorm.weight my size: torch.Size([768])\n",
      "bert name: cls.predictions.transform.LayerNorm.weight bert size: torch.Size([768])\n",
      "my name: lnorm.bias my size: torch.Size([768])\n",
      "bert name: cls.predictions.transform.LayerNorm.bias bert size: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "def copy_weights_from_bert(my_bert: BERTLanguageModel, bert: transformers.models.bert.modeling_bert.BertForMaskedLM) -> BERTLanguageModel:\n",
    "    '''\n",
    "    Copy over the weights from BERT to my BERT.\n",
    "    '''\n",
    "    my_list = list(my_bert.named_parameters())\n",
    "    my_list_rearranged = my_list[1:-4] + [my_list[0]] + my_list[-4:]\n",
    "    pretrained_dict = dict(bert.named_parameters())\n",
    "\n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    state_dict = {}\n",
    "\n",
    "    for (my_name, my_param), (pt_name, pt_param) in zip(my_list_rearranged, pretrained_dict.items()):\n",
    "        print(f\"my name: {my_name} my size: {my_param.shape}\")\n",
    "        print(f\"bert name: {pt_name} bert size: {pt_param.shape}\")\n",
    "        state_dict[my_name] = pt_param\n",
    "        \n",
    "    my_bert.load_state_dict(state_dict)\n",
    "    return my_bert\n",
    "\n",
    "my_bert = copy_weights_from_bert(my_bert, bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Former President of the United States of America, George[MASK][MASK]\n",
      "Model predicted: \n",
      " W Washington Bush Wallace Dewey Polk Patton H Marshall Buchanan Clinton C G Carter E\n",
      ";.!? |... Johnson Press Brown Carter Anderson III Smith Jones Thompson\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, text: str, k=15) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Return a list of k strings for each [MASK] in the input.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(text=text, return_tensors=\"pt\")\n",
    "    res = model(tokens)\n",
    "    \n",
    "    mask_predictions = []\n",
    "    for n, input_id in enumerate(tokens.squeeze()):\n",
    "        if input_id == tokenizer.mask_token_id:\n",
    "            logits = res[0, n]\n",
    "            top_logits_indices = t.topk(logits, k).indices\n",
    "            predictions = tokenizer.decode(top_logits_indices)\n",
    "            mask_predictions.append(predictions)\n",
    "    \n",
    "    return mask_predictions\n",
    "\n",
    "def test_bert_prediction(predict, model, tokenizer):\n",
    "\n",
    "    text = \"Former President of the United States of America, George[MASK][MASK]\"\n",
    "    predictions = predict(model, tokenizer, text)\n",
    "    print(f\"Prompt: {text}\")\n",
    "    print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n",
    "    assert \"Washington\" in predictions[0]\n",
    "    assert \"Bush\" in predictions[0]\n",
    "\n",
    "test_bert_prediction(predict, my_bert, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
